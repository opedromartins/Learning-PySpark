[
  {
    "objectID": "creating-dataframes.html",
    "href": "creating-dataframes.html",
    "title": "Creating Dataframes",
    "section": "",
    "text": "Based on WafaStudies PySpark tutorial."
  },
  {
    "objectID": "creating-dataframes.html#imports",
    "href": "creating-dataframes.html#imports",
    "title": "Creating Dataframes",
    "section": "Imports",
    "text": "Imports\n\nimport findspark\nfindspark.init()\n\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\nspark = SparkSession.builder\\\n                    .appName('Spark')\\\n                    .master(\"local[*]\")\\\n                    .getOrCreate()\n\nyour 131072x1 screen size is bogus. expect trouble\n23/10/25 15:10:15 WARN Utils: Your hostname, PC resolves to a loopback address: 127.0.1.1; using 172.29.148.244 instead (on interface eth0)\n23/10/25 15:10:15 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n23/10/25 15:10:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable"
  },
  {
    "objectID": "creating-dataframes.html#create-dataframe",
    "href": "creating-dataframes.html#create-dataframe",
    "title": "Creating Dataframes",
    "section": "Create Dataframe",
    "text": "Create Dataframe\n\nhelp(spark.createDataFrame)\n\nHelp on method createDataFrame in module pyspark.sql.session:\n\ncreateDataFrame(data: Union[pyspark.rdd.RDD[Any], Iterable[Any], ForwardRef('PandasDataFrameLike'), ForwardRef('ArrayLike')], schema: Union[pyspark.sql.types.AtomicType, pyspark.sql.types.StructType, str, NoneType] = None, samplingRatio: Optional[float] = None, verifySchema: bool = True) -&gt; pyspark.sql.dataframe.DataFrame method of pyspark.sql.session.SparkSession instance\n    Creates a :class:`DataFrame` from an :class:`RDD`, a list, a :class:`pandas.DataFrame`\n    or a :class:`numpy.ndarray`.\n    \n    .. versionadded:: 2.0.0\n    \n    .. versionchanged:: 3.4.0\n        Supports Spark Connect.\n    \n    Parameters\n    ----------\n    data : :class:`RDD` or iterable\n        an RDD of any kind of SQL data representation (:class:`Row`,\n        :class:`tuple`, ``int``, ``boolean``, etc.), or :class:`list`,\n        :class:`pandas.DataFrame` or :class:`numpy.ndarray`.\n    schema : :class:`pyspark.sql.types.DataType`, str or list, optional\n        a :class:`pyspark.sql.types.DataType` or a datatype string or a list of\n        column names, default is None. The data type string format equals to\n        :class:`pyspark.sql.types.DataType.simpleString`, except that top level struct type can\n        omit the ``struct&lt;&gt;``.\n    \n        When ``schema`` is a list of column names, the type of each column\n        will be inferred from ``data``.\n    \n        When ``schema`` is ``None``, it will try to infer the schema (column names and types)\n        from ``data``, which should be an RDD of either :class:`Row`,\n        :class:`namedtuple`, or :class:`dict`.\n    \n        When ``schema`` is :class:`pyspark.sql.types.DataType` or a datatype string, it must\n        match the real data, or an exception will be thrown at runtime. If the given schema is\n        not :class:`pyspark.sql.types.StructType`, it will be wrapped into a\n        :class:`pyspark.sql.types.StructType` as its only field, and the field name will be\n        \"value\". Each record will also be wrapped into a tuple, which can be converted to row\n        later.\n    samplingRatio : float, optional\n        the sample ratio of rows used for inferring. The first few rows will be used\n        if ``samplingRatio`` is ``None``.\n    verifySchema : bool, optional\n        verify data types of every row against schema. Enabled by default.\n    \n        .. versionadded:: 2.1.0\n    \n    Returns\n    -------\n    :class:`DataFrame`\n    \n    Notes\n    -----\n    Usage with `spark.sql.execution.arrow.pyspark.enabled=True` is experimental.\n    \n    Examples\n    --------\n    Create a DataFrame from a list of tuples.\n    \n    &gt;&gt;&gt; spark.createDataFrame([('Alice', 1)]).show()\n    +-----+---+\n    |   _1| _2|\n    +-----+---+\n    |Alice|  1|\n    +-----+---+\n    \n    Create a DataFrame from a list of dictionaries.\n    \n    &gt;&gt;&gt; d = [{'name': 'Alice', 'age': 1}]\n    &gt;&gt;&gt; spark.createDataFrame(d).show()\n    +---+-----+\n    |age| name|\n    +---+-----+\n    |  1|Alice|\n    +---+-----+\n    \n    Create a DataFrame with column names specified.\n    \n    &gt;&gt;&gt; spark.createDataFrame([('Alice', 1)], ['name', 'age']).show()\n    +-----+---+\n    | name|age|\n    +-----+---+\n    |Alice|  1|\n    +-----+---+\n    \n    Create a DataFrame with the explicit schema specified.\n    \n    &gt;&gt;&gt; from pyspark.sql.types import *\n    &gt;&gt;&gt; schema = StructType([\n    ...    StructField(\"name\", StringType(), True),\n    ...    StructField(\"age\", IntegerType(), True)])\n    &gt;&gt;&gt; spark.createDataFrame([('Alice', 1)], schema).show()\n    +-----+---+\n    | name|age|\n    +-----+---+\n    |Alice|  1|\n    +-----+---+\n    \n    Create a DataFrame with the schema in DDL formatted string.\n    \n    &gt;&gt;&gt; spark.createDataFrame([('Alice', 1)], \"name: string, age: int\").show()\n    +-----+---+\n    | name|age|\n    +-----+---+\n    |Alice|  1|\n    +-----+---+\n    \n    Create an empty DataFrame.\n    When initializing an empty DataFrame in PySpark, it's mandatory to specify its schema,\n    as the DataFrame lacks data from which the schema can be inferred.\n    \n    &gt;&gt;&gt; spark.createDataFrame([], \"name: string, age: int\").show()\n    +----+---+\n    |name|age|\n    +----+---+\n    +----+---+\n    \n    Create a DataFrame from Row objects.\n    \n    &gt;&gt;&gt; from pyspark.sql import Row\n    &gt;&gt;&gt; Person = Row('name', 'age')\n    &gt;&gt;&gt; df = spark.createDataFrame([Person(\"Alice\", 1)])\n    &gt;&gt;&gt; df.show()\n    +-----+---+\n    | name|age|\n    +-----+---+\n    |Alice|  1|\n    +-----+---+\n    \n    Create a DataFrame from a pandas DataFrame.\n    \n    &gt;&gt;&gt; spark.createDataFrame(df.toPandas()).show()  # doctest: +SKIP\n    +-----+---+\n    | name|age|\n    +-----+---+\n    |Alice|  1|\n    +-----+---+\n    &gt;&gt;&gt; spark.createDataFrame(pandas.DataFrame([[1, 2]])).collect()  # doctest: +SKIP\n    +---+---+\n    |  0|  1|\n    +---+---+\n    |  1|  2|\n    +---+---+\n\n\n\n\ndata = [(1,'Pedro'), (2, 'Guilherme')]\ndf = spark.createDataFrame(data=data)\n\ndf.show()\ndf.printSchema()\n\n                                                                                \n\n\n+---+---------+\n| _1|       _2|\n+---+---------+\n|  1|    Pedro|\n|  2|Guilherme|\n+---+---------+\n\nroot\n |-- _1: long (nullable = true)\n |-- _2: string (nullable = true)\n\n\n\n\ndata = [(1,'Pedro'), (2, 'Guilherme')]\ndf = spark.createDataFrame(data=data, schema=[\"id\", \"name\"])\n\ndf.show()\ndf.printSchema()\n\n+---+---------+\n| id|     name|\n+---+---------+\n|  1|    Pedro|\n|  2|Guilherme|\n+---+---------+\n\nroot\n |-- id: long (nullable = true)\n |-- name: string (nullable = true)\n\n\n\n\nfrom pyspark.sql.types import *\nhelp(StructType)\n\nHelp on class StructType in module pyspark.sql.types:\n\nclass StructType(DataType)\n |  StructType(fields: Optional[List[pyspark.sql.types.StructField]] = None)\n |  \n |  Struct type, consisting of a list of :class:`StructField`.\n |  \n |  This is the data type representing a :class:`Row`.\n |  \n |  Iterating a :class:`StructType` will iterate over its :class:`StructField`\\s.\n |  A contained :class:`StructField` can be accessed by its name or position.\n |  \n |  Examples\n |  --------\n |  &gt;&gt;&gt; from pyspark.sql.types import *\n |  &gt;&gt;&gt; struct1 = StructType([StructField(\"f1\", StringType(), True)])\n |  &gt;&gt;&gt; struct1[\"f1\"]\n |  StructField('f1', StringType(), True)\n |  &gt;&gt;&gt; struct1[0]\n |  StructField('f1', StringType(), True)\n |  \n |  &gt;&gt;&gt; struct1 = StructType([StructField(\"f1\", StringType(), True)])\n |  &gt;&gt;&gt; struct2 = StructType([StructField(\"f1\", StringType(), True)])\n |  &gt;&gt;&gt; struct1 == struct2\n |  True\n |  &gt;&gt;&gt; struct1 = StructType([StructField(\"f1\", CharType(10), True)])\n |  &gt;&gt;&gt; struct2 = StructType([StructField(\"f1\", CharType(10), True)])\n |  &gt;&gt;&gt; struct1 == struct2\n |  True\n |  &gt;&gt;&gt; struct1 = StructType([StructField(\"f1\", VarcharType(10), True)])\n |  &gt;&gt;&gt; struct2 = StructType([StructField(\"f1\", VarcharType(10), True)])\n |  &gt;&gt;&gt; struct1 == struct2\n |  True\n |  &gt;&gt;&gt; struct1 = StructType([StructField(\"f1\", StringType(), True)])\n |  &gt;&gt;&gt; struct2 = StructType([StructField(\"f1\", StringType(), True),\n |  ...     StructField(\"f2\", IntegerType(), False)])\n |  &gt;&gt;&gt; struct1 == struct2\n |  False\n |  \n |  The below example demonstrates how to create a DataFrame based on a struct created\n |  using class:`StructType` and class:`StructField`:\n |  \n |  &gt;&gt;&gt; data = [(\"Alice\", [\"Java\", \"Scala\"]), (\"Bob\", [\"Python\", \"Scala\"])]\n |  &gt;&gt;&gt; schema = StructType([\n |  ...     StructField(\"name\", StringType()),\n |  ...     StructField(\"languagesSkills\", ArrayType(StringType())),\n |  ... ])\n |  &gt;&gt;&gt; df = spark.createDataFrame(data=data, schema=schema)\n |  &gt;&gt;&gt; df.printSchema()\n |  root\n |   |-- name: string (nullable = true)\n |   |-- languagesSkills: array (nullable = true)\n |   |    |-- element: string (containsNull = true)\n |  &gt;&gt;&gt; df.show()\n |  +-----+---------------+\n |  | name|languagesSkills|\n |  +-----+---------------+\n |  |Alice|  [Java, Scala]|\n |  |  Bob|[Python, Scala]|\n |  +-----+---------------+\n |  \n |  Method resolution order:\n |      StructType\n |      DataType\n |      builtins.object\n |  \n |  Methods defined here:\n |  \n |  __getitem__(self, key: Union[str, int]) -&gt; pyspark.sql.types.StructField\n |      Access fields by name or slice.\n |  \n |  __init__(self, fields: Optional[List[pyspark.sql.types.StructField]] = None)\n |      Initialize self.  See help(type(self)) for accurate signature.\n |  \n |  __iter__(self) -&gt; Iterator[pyspark.sql.types.StructField]\n |      Iterate the fields\n |  \n |  __len__(self) -&gt; int\n |      Return the number of fields.\n |  \n |  __repr__(self) -&gt; str\n |      Return repr(self).\n |  \n |  add(self, field: Union[str, pyspark.sql.types.StructField], data_type: Union[str, pyspark.sql.types.DataType, NoneType] = None, nullable: bool = True, metadata: Optional[Dict[str, Any]] = None) -&gt; 'StructType'\n |      Construct a :class:`StructType` by adding new elements to it, to define the schema.\n |      The method accepts either:\n |      \n |          a) A single parameter which is a :class:`StructField` object.\n |          b) Between 2 and 4 parameters as (name, data_type, nullable (optional),\n |             metadata(optional). The data_type parameter may be either a String or a\n |             :class:`DataType` object.\n |      \n |      Parameters\n |      ----------\n |      field : str or :class:`StructField`\n |          Either the name of the field or a :class:`StructField` object\n |      data_type : :class:`DataType`, optional\n |          If present, the DataType of the :class:`StructField` to create\n |      nullable : bool, optional\n |          Whether the field to add should be nullable (default True)\n |      metadata : dict, optional\n |          Any additional metadata (default None)\n |      \n |      Returns\n |      -------\n |      :class:`StructType`\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; from pyspark.sql.types import IntegerType, StringType, StructField, StructType\n |      &gt;&gt;&gt; struct1 = StructType().add(\"f1\", StringType(), True).add(\"f2\", StringType(), True, None)\n |      &gt;&gt;&gt; struct2 = StructType([StructField(\"f1\", StringType(), True),\n |      ...     StructField(\"f2\", StringType(), True, None)])\n |      &gt;&gt;&gt; struct1 == struct2\n |      True\n |      &gt;&gt;&gt; struct1 = StructType().add(StructField(\"f1\", StringType(), True))\n |      &gt;&gt;&gt; struct2 = StructType([StructField(\"f1\", StringType(), True)])\n |      &gt;&gt;&gt; struct1 == struct2\n |      True\n |      &gt;&gt;&gt; struct1 = StructType().add(\"f1\", \"string\", True)\n |      &gt;&gt;&gt; struct2 = StructType([StructField(\"f1\", StringType(), True)])\n |      &gt;&gt;&gt; struct1 == struct2\n |      True\n |  \n |  fieldNames(self) -&gt; List[str]\n |      Returns all field names in a list.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; from pyspark.sql.types import StringType, StructField, StructType\n |      &gt;&gt;&gt; struct = StructType([StructField(\"f1\", StringType(), True)])\n |      &gt;&gt;&gt; struct.fieldNames()\n |      ['f1']\n |  \n |  fromInternal(self, obj: Tuple) -&gt; 'Row'\n |      Converts an internal SQL object into a native Python object.\n |  \n |  jsonValue(self) -&gt; Dict[str, Any]\n |  \n |  needConversion(self) -&gt; bool\n |      Does this type needs conversion between Python object and internal SQL object.\n |      \n |      This is used to avoid the unnecessary conversion for ArrayType/MapType/StructType.\n |  \n |  simpleString(self) -&gt; str\n |  \n |  toInternal(self, obj: Tuple) -&gt; Tuple\n |      Converts a Python object into an internal SQL object.\n |  \n |  ----------------------------------------------------------------------\n |  Class methods defined here:\n |  \n |  fromJson(json: Dict[str, Any]) -&gt; 'StructType' from builtins.type\n |      Constructs :class:`StructType` from a schema defined in JSON format.\n |      \n |      Below is a JSON schema it must adhere to::\n |      \n |          {\n |            \"title\":\"StructType\",\n |            \"description\":\"Schema of StructType in json format\",\n |            \"type\":\"object\",\n |            \"properties\":{\n |               \"fields\":{\n |                  \"description\":\"Array of struct fields\",\n |                  \"type\":\"array\",\n |                  \"items\":{\n |                      \"type\":\"object\",\n |                      \"properties\":{\n |                         \"name\":{\n |                            \"description\":\"Name of the field\",\n |                            \"type\":\"string\"\n |                         },\n |                         \"type\":{\n |                            \"description\": \"Type of the field. Can either be\n |                                            another nested StructType or primitive type\",\n |                            \"type\":\"object/string\"\n |                         },\n |                         \"nullable\":{\n |                            \"description\":\"If nulls are allowed\",\n |                            \"type\":\"boolean\"\n |                         },\n |                         \"metadata\":{\n |                            \"description\":\"Additional metadata to supply\",\n |                            \"type\":\"object\"\n |                         },\n |                         \"required\":[\n |                            \"name\",\n |                            \"type\",\n |                            \"nullable\",\n |                            \"metadata\"\n |                         ]\n |                      }\n |                 }\n |              }\n |           }\n |         }\n |      \n |      Parameters\n |      ----------\n |      json : dict or a dict-like object e.g. JSON object\n |          This \"dict\" must have \"fields\" key that returns an array of fields\n |          each of which must have specific keys (name, type, nullable, metadata).\n |      \n |      Returns\n |      -------\n |      :class:`StructType`\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; json_str = '''\n |      ...  {\n |      ...      \"fields\": [\n |      ...          {\n |      ...              \"metadata\": {},\n |      ...              \"name\": \"Person\",\n |      ...              \"nullable\": true,\n |      ...              \"type\": {\n |      ...                  \"fields\": [\n |      ...                      {\n |      ...                          \"metadata\": {},\n |      ...                          \"name\": \"name\",\n |      ...                          \"nullable\": false,\n |      ...                          \"type\": \"string\"\n |      ...                      },\n |      ...                      {\n |      ...                          \"metadata\": {},\n |      ...                          \"name\": \"surname\",\n |      ...                          \"nullable\": false,\n |      ...                          \"type\": \"string\"\n |      ...                      }\n |      ...                  ],\n |      ...                  \"type\": \"struct\"\n |      ...              }\n |      ...          }\n |      ...      ],\n |      ...      \"type\": \"struct\"\n |      ...  }\n |      ...  '''\n |      &gt;&gt;&gt; import json\n |      &gt;&gt;&gt; scheme = StructType.fromJson(json.loads(json_str))\n |      &gt;&gt;&gt; scheme.simpleString()\n |      'struct&lt;Person:struct&lt;name:string,surname:string&gt;&gt;'\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from DataType:\n |  \n |  __eq__(self, other: Any) -&gt; bool\n |      Return self==value.\n |  \n |  __hash__(self) -&gt; int\n |      Return hash(self).\n |  \n |  __ne__(self, other: Any) -&gt; bool\n |      Return self!=value.\n |  \n |  json(self) -&gt; str\n |  \n |  ----------------------------------------------------------------------\n |  Class methods inherited from DataType:\n |  \n |  typeName() -&gt; str from builtins.type\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors inherited from DataType:\n |  \n |  __dict__\n |      dictionary for instance variables (if defined)\n |  \n |  __weakref__\n |      list of weak references to the object (if defined)\n\n\n\n\ndata = [(1,'Pedro'), (2, 'Guilherme')]\nschema = StructType([StructField(name='id', dataType=IntegerType()),\n                     StructField(name='name', dataType=StringType())])\n\ndf = spark.createDataFrame(data=data, schema=schema)\n\ndf.show()\ndf.printSchema()\n\n+---+---------+\n| id|     name|\n+---+---------+\n|  1|    Pedro|\n|  2|Guilherme|\n+---+---------+\n\nroot\n |-- id: integer (nullable = true)\n |-- name: string (nullable = true)\n\n\n\n\ndata = [{'id':1, 'name':'Pedro'},\n        {'id':2, 'name':'Guilherme'}]\n\ndf = spark.createDataFrame(data=data)\n\ndf.show()\ndf.printSchema()\n\n+---+---------+\n| id|     name|\n+---+---------+\n|  1|    Pedro|\n|  2|Guilherme|\n+---+---------+\n\nroot\n |-- id: long (nullable = true)\n |-- name: string (nullable = true)"
  },
  {
    "objectID": "reading-csv-files.html",
    "href": "reading-csv-files.html",
    "title": "Reading CSV files",
    "section": "",
    "text": "Based on WafaStudies PySpark tutorial."
  },
  {
    "objectID": "reading-csv-files.html#imports",
    "href": "reading-csv-files.html#imports",
    "title": "Reading CSV files",
    "section": "Imports",
    "text": "Imports\n\nimport findspark\nfindspark.init()\n\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\nspark = SparkSession.builder\\\n                    .appName('Spark')\\\n                    .master(\"local[*]\")\\\n                    .getOrCreate()\n\nyour 131072x1 screen size is bogus. expect trouble\n23/10/25 15:11:16 WARN Utils: Your hostname, PC resolves to a loopback address: 127.0.1.1; using 172.29.148.244 instead (on interface eth0)\n23/10/25 15:11:16 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n23/10/25 15:11:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n23/10/25 15:11:18 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041."
  },
  {
    "objectID": "reading-csv-files.html#generate-data",
    "href": "reading-csv-files.html#generate-data",
    "title": "Reading CSV files",
    "section": "Generate data",
    "text": "Generate data\n\n!mkdir data\n\n\nimport csv\nimport random\nfrom faker import Faker\n\nfaker = Faker()\n\nwith open('data/employees1.csv', 'w', newline='') as csvfile:\n    fieldnames = ['id', 'name', 'gender', 'salary']\n    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n    writer.writeheader()\n\n    for id in range(1, 6):\n        name = faker.name()\n        gender = random.choice(['Male', 'Female'])\n        salary = random.randint(1000, 10000)\n\n        writer.writerow({'id': id, 'name': name, 'gender': gender, 'salary': salary})\n\nwith open('data/employees2.csv', 'w', newline='') as csvfile:\n    fieldnames = ['id', 'name', 'gender', 'salary']\n    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n\n    writer.writeheader()\n\n    for id in range(1, 6):\n        name = faker.name()\n        gender = random.choice(['Male', 'Female'])\n        salary = random.randint(1000, 10000)\n\n        writer.writerow({'id': id, 'name': name, 'gender': gender, 'salary': salary})"
  },
  {
    "objectID": "reading-csv-files.html#reading-csv-files",
    "href": "reading-csv-files.html#reading-csv-files",
    "title": "Reading CSV files",
    "section": "Reading CSV files",
    "text": "Reading CSV files\n\nhelp(spark.read.csv)\n\nHelp on method csv in module pyspark.sql.readwriter:\n\ncsv(path: Union[str, List[str]], schema: Union[pyspark.sql.types.StructType, str, NoneType] = None, sep: Optional[str] = None, encoding: Optional[str] = None, quote: Optional[str] = None, escape: Optional[str] = None, comment: Optional[str] = None, header: Union[bool, str, NoneType] = None, inferSchema: Union[bool, str, NoneType] = None, ignoreLeadingWhiteSpace: Union[bool, str, NoneType] = None, ignoreTrailingWhiteSpace: Union[bool, str, NoneType] = None, nullValue: Optional[str] = None, nanValue: Optional[str] = None, positiveInf: Optional[str] = None, negativeInf: Optional[str] = None, dateFormat: Optional[str] = None, timestampFormat: Optional[str] = None, maxColumns: Union[str, int, NoneType] = None, maxCharsPerColumn: Union[str, int, NoneType] = None, maxMalformedLogPerPartition: Union[str, int, NoneType] = None, mode: Optional[str] = None, columnNameOfCorruptRecord: Optional[str] = None, multiLine: Union[bool, str, NoneType] = None, charToEscapeQuoteEscaping: Optional[str] = None, samplingRatio: Union[str, float, NoneType] = None, enforceSchema: Union[bool, str, NoneType] = None, emptyValue: Optional[str] = None, locale: Optional[str] = None, lineSep: Optional[str] = None, pathGlobFilter: Union[bool, str, NoneType] = None, recursiveFileLookup: Union[bool, str, NoneType] = None, modifiedBefore: Union[bool, str, NoneType] = None, modifiedAfter: Union[bool, str, NoneType] = None, unescapedQuoteHandling: Optional[str] = None) -&gt; 'DataFrame' method of pyspark.sql.readwriter.DataFrameReader instance\n    Loads a CSV file and returns the result as a  :class:`DataFrame`.\n    \n    This function will go through the input once to determine the input schema if\n    ``inferSchema`` is enabled. To avoid going through the entire data once, disable\n    ``inferSchema`` option or specify the schema explicitly using ``schema``.\n    \n    .. versionadded:: 2.0.0\n    \n    .. versionchanged:: 3.4.0\n        Supports Spark Connect.\n    \n    Parameters\n    ----------\n    path : str or list\n        string, or list of strings, for input path(s),\n        or RDD of Strings storing CSV rows.\n    schema : :class:`pyspark.sql.types.StructType` or str, optional\n        an optional :class:`pyspark.sql.types.StructType` for the input schema\n        or a DDL-formatted string (For example ``col0 INT, col1 DOUBLE``).\n    \n    Other Parameters\n    ----------------\n    Extra options\n        For the extra options, refer to\n        `Data Source Option &lt;https://spark.apache.org/docs/latest/sql-data-sources-csv.html#data-source-option&gt;`_\n        for the version you use.\n    \n        .. # noqa\n    \n    Examples\n    --------\n    Write a DataFrame into a CSV file and read it back.\n    \n    &gt;&gt;&gt; import tempfile\n    &gt;&gt;&gt; with tempfile.TemporaryDirectory() as d:\n    ...     # Write a DataFrame into a CSV file\n    ...     df = spark.createDataFrame([{\"age\": 100, \"name\": \"Hyukjin Kwon\"}])\n    ...     df.write.mode(\"overwrite\").format(\"csv\").save(d)\n    ...\n    ...     # Read the CSV file as a DataFrame with 'nullValue' option set to 'Hyukjin Kwon'.\n    ...     spark.read.csv(d, schema=df.schema, nullValue=\"Hyukjin Kwon\").show()\n    +---+----+\n    |age|name|\n    +---+----+\n    |100|NULL|\n    +---+----+\n\n\n\n\ndf = spark.read.csv(path='data/employees1.csv')\ndf.show()\ndf.printSchema()\n\n+---+----------------+------+------+\n|_c0|             _c1|   _c2|   _c3|\n+---+----------------+------+------+\n| id|            name|gender|salary|\n|  1|      Kara Moyer|  Male|  3265|\n|  2|    Kathryn Bell|  Male|  4657|\n|  3|   Gerald Newman|  Male|  4000|\n|  4|Angela Rodriguez|Female|  2596|\n|  5|     Terry Smith|Female|  3685|\n+---+----------------+------+------+\n\nroot\n |-- _c0: string (nullable = true)\n |-- _c1: string (nullable = true)\n |-- _c2: string (nullable = true)\n |-- _c3: string (nullable = true)\n\n\n\nBy default, spark read csv without header and all datatypes as string.\nTo avoid it, we use:\nheader=True: first line will be taken as header\ninferSchema=True: spark will infer the datatypes of each column\n\ndf = spark.read.csv(path='data/employees1.csv', header=True, inferSchema=True)\ndf.show()\ndf.printSchema()\n\n+---+----------------+------+------+\n| id|            name|gender|salary|\n+---+----------------+------+------+\n|  1|      Kara Moyer|  Male|  3265|\n|  2|    Kathryn Bell|  Male|  4657|\n|  3|   Gerald Newman|  Male|  4000|\n|  4|Angela Rodriguez|Female|  2596|\n|  5|     Terry Smith|Female|  3685|\n+---+----------------+------+------+\n\nroot\n |-- id: integer (nullable = true)\n |-- name: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: integer (nullable = true)\n\n\n\ninferSchema takes some time and processing power, so we can tell spark the schema:\n\nschema = 'id integer, name string, gender string, salary double'\n\n\ndf = spark.read.csv(path='data/employees1.csv', header=True, schema=schema)\n\ndf.show()\ndf.printSchema()\n\n+---+----------------+------+------+\n| id|            name|gender|salary|\n+---+----------------+------+------+\n|  1|      Kara Moyer|  Male|3265.0|\n|  2|    Kathryn Bell|  Male|4657.0|\n|  3|   Gerald Newman|  Male|4000.0|\n|  4|Angela Rodriguez|Female|2596.0|\n|  5|     Terry Smith|Female|3685.0|\n+---+----------------+------+------+\n\nroot\n |-- id: integer (nullable = true)\n |-- name: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: double (nullable = true)\n\n\n\nWe can also read multiple files in one dataframe:\n\ndf = spark.read.csv(path=['data/employees1.csv', 'data/employees2.csv'], header=True, schema=schema)\n\ndf.show()\ndf.printSchema()\n\n+---+----------------+------+------+\n| id|            name|gender|salary|\n+---+----------------+------+------+\n|  1|   Brandon Davis|  Male|1786.0|\n|  2|   Amanda Hansen|  Male|9218.0|\n|  3|Valerie Peterson|Female|4119.0|\n|  4|    Robert Mason|  Male|7547.0|\n|  5|    James Thomas|Female|4181.0|\n|  1|      Kara Moyer|  Male|3265.0|\n|  2|    Kathryn Bell|  Male|4657.0|\n|  3|   Gerald Newman|  Male|4000.0|\n|  4|Angela Rodriguez|Female|2596.0|\n|  5|     Terry Smith|Female|3685.0|\n+---+----------------+------+------+\n\nroot\n |-- id: integer (nullable = true)\n |-- name: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: double (nullable = true)\n\n\n\nIf all the files are in the same folder, it’s possible to use the folder path:\n\ndf = spark.read.csv(path=['data/'], header=True, schema=schema)\n\ndf.show()\ndf.printSchema()\n\n+---+----------------+------+------+\n| id|            name|gender|salary|\n+---+----------------+------+------+\n|  1|   Brandon Davis|  Male|1786.0|\n|  2|   Amanda Hansen|  Male|9218.0|\n|  3|Valerie Peterson|Female|4119.0|\n|  4|    Robert Mason|  Male|7547.0|\n|  5|    James Thomas|Female|4181.0|\n|  1|      Kara Moyer|  Male|3265.0|\n|  2|    Kathryn Bell|  Male|4657.0|\n|  3|   Gerald Newman|  Male|4000.0|\n|  4|Angela Rodriguez|Female|2596.0|\n|  5|     Terry Smith|Female|3685.0|\n+---+----------------+------+------+\n\nroot\n |-- id: integer (nullable = true)\n |-- name: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: double (nullable = true)"
  },
  {
    "objectID": "writing-dataframe-to-csv.html",
    "href": "writing-dataframe-to-csv.html",
    "title": "Writing dataframe to CSV",
    "section": "",
    "text": "Based on WafaStudies PySpark tutorial."
  },
  {
    "objectID": "writing-dataframe-to-csv.html#imports",
    "href": "writing-dataframe-to-csv.html#imports",
    "title": "Writing dataframe to CSV",
    "section": "Imports",
    "text": "Imports\n\nimport findspark\nfindspark.init()\n\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\nspark = SparkSession.builder\\\n                    .appName('Spark')\\\n                    .master(\"local[*]\")\\\n                    .getOrCreate()\n\nfrom pyspark.sql import dataframe\n\nyour 131072x1 screen size is bogus. expect trouble\n23/10/25 15:11:26 WARN Utils: Your hostname, PC resolves to a loopback address: 127.0.1.1; using 172.29.148.244 instead (on interface eth0)\n23/10/25 15:11:26 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n23/10/25 15:11:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n23/10/25 15:11:28 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n23/10/25 15:11:28 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042."
  },
  {
    "objectID": "writing-dataframe-to-csv.html#writing-dataframe-to-csv-files",
    "href": "writing-dataframe-to-csv.html#writing-dataframe-to-csv-files",
    "title": "Writing dataframe to CSV",
    "section": "Writing dataframe to CSV files",
    "text": "Writing dataframe to CSV files\n\nhelp(dataframe.DataFrame.write)\n\nHelp on property:\n\n    Interface for saving the content of the non-streaming :class:`DataFrame` out into external\n    storage.\n    \n    .. versionadded:: 1.4.0\n    \n    .. versionchanged:: 3.4.0\n        Supports Spark Connect.\n    \n    Returns\n    -------\n    :class:`DataFrameWriter`\n    \n    Examples\n    --------\n    &gt;&gt;&gt; df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n    &gt;&gt;&gt; type(df.write)\n    &lt;class '...readwriter.DataFrameWriter'&gt;\n    \n    Write the DataFrame as a table.\n    \n    &gt;&gt;&gt; _ = spark.sql(\"DROP TABLE IF EXISTS tab2\")\n    &gt;&gt;&gt; df.write.saveAsTable(\"tab2\")\n    &gt;&gt;&gt; _ = spark.sql(\"DROP TABLE tab2\")\n\n\n\nLet’s start creating a dataframe\n\ndf = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n\ndf.show()\ndf.printSchema()\n\n                                                                                \n\n\n+---+-----+\n|age| name|\n+---+-----+\n|  2|Alice|\n|  5|  Bob|\n+---+-----+\n\nroot\n |-- age: long (nullable = true)\n |-- name: string (nullable = true)\n\n\n\n\nhelp(df.write.csv)\n\nHelp on method csv in module pyspark.sql.readwriter:\n\ncsv(path: str, mode: Optional[str] = None, compression: Optional[str] = None, sep: Optional[str] = None, quote: Optional[str] = None, escape: Optional[str] = None, header: Union[bool, str, NoneType] = None, nullValue: Optional[str] = None, escapeQuotes: Union[bool, str, NoneType] = None, quoteAll: Union[bool, str, NoneType] = None, dateFormat: Optional[str] = None, timestampFormat: Optional[str] = None, ignoreLeadingWhiteSpace: Union[bool, str, NoneType] = None, ignoreTrailingWhiteSpace: Union[bool, str, NoneType] = None, charToEscapeQuoteEscaping: Optional[str] = None, encoding: Optional[str] = None, emptyValue: Optional[str] = None, lineSep: Optional[str] = None) -&gt; None method of pyspark.sql.readwriter.DataFrameWriter instance\n    Saves the content of the :class:`DataFrame` in CSV format at the specified path.\n    \n    .. versionadded:: 2.0.0\n    \n    .. versionchanged:: 3.4.0\n        Supports Spark Connect.\n    \n    Parameters\n    ----------\n    path : str\n        the path in any Hadoop supported file system\n    mode : str, optional\n        specifies the behavior of the save operation when data already exists.\n    \n        * ``append``: Append contents of this :class:`DataFrame` to existing data.\n        * ``overwrite``: Overwrite existing data.\n        * ``ignore``: Silently ignore this operation if data already exists.\n        * ``error`` or ``errorifexists`` (default case): Throw an exception if data already \\\n            exists.\n    \n    Other Parameters\n    ----------------\n    Extra options\n        For the extra options, refer to\n        `Data Source Option &lt;https://spark.apache.org/docs/latest/sql-data-sources-csv.html#data-source-option&gt;`_\n        for the version you use.\n    \n        .. # noqa\n    \n    Examples\n    --------\n    Write a DataFrame into a CSV file and read it back.\n    \n    &gt;&gt;&gt; import tempfile\n    &gt;&gt;&gt; with tempfile.TemporaryDirectory() as d:\n    ...     # Write a DataFrame into a CSV file\n    ...     df = spark.createDataFrame([{\"age\": 100, \"name\": \"Hyukjin Kwon\"}])\n    ...     df.write.csv(d, mode=\"overwrite\")\n    ...\n    ...     # Read the CSV file as a DataFrame with 'nullValue' option set to 'Hyukjin Kwon'.\n    ...     spark.read.schema(df.schema).format(\"csv\").option(\n    ...         \"nullValue\", \"Hyukjin Kwon\").load(d).show()\n    +---+----+\n    |age|name|\n    +---+----+\n    |100|NULL|\n    +---+----+\n\n\n\n\ndf.write\\\n.csv(\"df_csv\")\n\nLet’s check the written file:\n\nspark.read.schema(df.schema)\\\n.format(\"csv\")\\\n.load(\"df_csv\").show()\n\n+---+-----+\n|age| name|\n+---+-----+\n|  2|Alice|\n|  5|  Bob|\n+---+-----+\n\n\n\nWhat if we want to change the dataframe?\n\ndf = spark.createDataFrame([(1, \"Goku\"), (2, \"Naruto\")], schema=[\"id\", \"name\"])\n\ndf.show()\ndf.printSchema()\n\n+---+------+\n| id|  name|\n+---+------+\n|  1|  Goku|\n|  2|Naruto|\n+---+------+\n\nroot\n |-- id: long (nullable = true)\n |-- name: string (nullable = true)\n\n\n\n\ndf.write\\\n.csv(\"df_csv\")\n\nAnalysisException: [PATH_ALREADY_EXISTS] Path file:/home/pedro-wsl/Learning-PySpark/nbs/df_csv already exists. Set mode as \"overwrite\" to overwrite the existing path.\n\n\nIt will give us an error because the file already exists, so we need to overwrite it:\n\ndf.write\\\n.csv(\"df_csv\", mode=\"overwrite\")\n\nLet’s check:\n\nspark.read.schema(df.schema)\\\n.format(\"csv\")\\\n.load(\"df_csv\").show()\n\n+---+------+\n| id|  name|\n+---+------+\n|  2|Naruto|\n|  1|  Goku|\n+---+------+\n\n\n\nAnd how to add more items to the file?\nLet’s create another dataframe:\n\ndf2 = spark.createDataFrame([(\"3\", \"Gojo\"), (\"4\", \"Kirito\")], schema=[\"id\", \"name\"])\n\ndf2.show()\ndf2.printSchema()\n\n+---+------+\n| id|  name|\n+---+------+\n|  3|  Gojo|\n|  4|Kirito|\n+---+------+\n\nroot\n |-- id: string (nullable = true)\n |-- name: string (nullable = true)\n\n\n\nThen we have to append it to the file:\n\ndf2.write\\\n.csv(\"df_csv\", mode=\"append\")\n\n\nspark.read.schema(df.schema)\\\n.format(\"csv\")\\\n.load(\"df_csv\").show()\n\n+---+------+\n| id|  name|\n+---+------+\n|  2|Naruto|\n|  4|Kirito|\n|  3|  Gojo|\n|  1|  Goku|\n+---+------+\n\n\n\n\nFile structure:\nLet’s check the csv file structure:\n\n!file -b df_csv\n\ndirectory\n\n\nIt’s a folder, let’s check it’s content:\n\n!ls -la df_csv\n\ntotal 52\ndrwxr-xr-x 2 pedro-wsl pedro-wsl 4096 Oct 25 15:13 .\ndrwxr-xr-x 5 pedro-wsl pedro-wsl 4096 Oct 25 15:13 ..\n-rw-r--r-- 1 pedro-wsl pedro-wsl    8 Oct 25 15:13 ._SUCCESS.crc\n-rw-r--r-- 1 pedro-wsl pedro-wsl    8 Oct 25 15:13 .part-00000-1d555ec3-1910-480c-b90f-23b1a97a974c-c000.csv.crc\n-rw-r--r-- 1 pedro-wsl pedro-wsl    8 Oct 25 15:13 .part-00000-ab8126fc-3fdb-44d1-8d0a-c7556dbe19c0-c000.csv.crc\n-rw-r--r-- 1 pedro-wsl pedro-wsl   12 Oct 25 15:13 .part-00005-1d555ec3-1910-480c-b90f-23b1a97a974c-c000.csv.crc\n-rw-r--r-- 1 pedro-wsl pedro-wsl   12 Oct 25 15:13 .part-00005-ab8126fc-3fdb-44d1-8d0a-c7556dbe19c0-c000.csv.crc\n-rw-r--r-- 1 pedro-wsl pedro-wsl   12 Oct 25 15:13 .part-00011-1d555ec3-1910-480c-b90f-23b1a97a974c-c000.csv.crc\n-rw-r--r-- 1 pedro-wsl pedro-wsl   12 Oct 25 15:13 .part-00011-ab8126fc-3fdb-44d1-8d0a-c7556dbe19c0-c000.csv.crc\n-rw-r--r-- 1 pedro-wsl pedro-wsl    0 Oct 25 15:13 _SUCCESS\n-rw-r--r-- 1 pedro-wsl pedro-wsl    0 Oct 25 15:13 part-00000-1d555ec3-1910-480c-b90f-23b1a97a974c-c000.csv\n-rw-r--r-- 1 pedro-wsl pedro-wsl    0 Oct 25 15:13 part-00000-ab8126fc-3fdb-44d1-8d0a-c7556dbe19c0-c000.csv\n-rw-r--r-- 1 pedro-wsl pedro-wsl    7 Oct 25 15:13 part-00005-1d555ec3-1910-480c-b90f-23b1a97a974c-c000.csv\n-rw-r--r-- 1 pedro-wsl pedro-wsl    7 Oct 25 15:13 part-00005-ab8126fc-3fdb-44d1-8d0a-c7556dbe19c0-c000.csv\n-rw-r--r-- 1 pedro-wsl pedro-wsl    9 Oct 25 15:13 part-00011-1d555ec3-1910-480c-b90f-23b1a97a974c-c000.csv\n-rw-r--r-- 1 pedro-wsl pedro-wsl    9 Oct 25 15:13 part-00011-ab8126fc-3fdb-44d1-8d0a-c7556dbe19c0-c000.csv\n\n\nIt’s divided in partitions\nThe number of partitions is the same number of rows we have on df\nThis happens because spark have a driver node that divide the workload between worker nodes, like:\n  Driver Node\n /   |    |   \\\nW1   W2   W3   W4\nWe can also specify the number of partitions we want:\n\nhelp(df.repartition)\n\nHelp on method repartition in module pyspark.sql.dataframe:\n\nrepartition(numPartitions: Union[int, ForwardRef('ColumnOrName')], *cols: 'ColumnOrName') -&gt; 'DataFrame' method of pyspark.sql.dataframe.DataFrame instance\n    Returns a new :class:`DataFrame` partitioned by the given partitioning expressions. The\n    resulting :class:`DataFrame` is hash partitioned.\n    \n    .. versionadded:: 1.3.0\n    \n    .. versionchanged:: 3.4.0\n        Supports Spark Connect.\n    \n    Parameters\n    ----------\n    numPartitions : int\n        can be an int to specify the target number of partitions or a Column.\n        If it is a Column, it will be used as the first partitioning column. If not specified,\n        the default number of partitions is used.\n    cols : str or :class:`Column`\n        partitioning columns.\n    \n        .. versionchanged:: 1.6.0\n           Added optional arguments to specify the partitioning columns. Also made numPartitions\n           optional if partitioning columns are specified.\n    \n    Returns\n    -------\n    :class:`DataFrame`\n        Repartitioned DataFrame.\n    \n    Examples\n    --------\n    &gt;&gt;&gt; df = spark.createDataFrame(\n    ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n    \n    Repartition the data into 10 partitions.\n    \n    &gt;&gt;&gt; df.repartition(10).rdd.getNumPartitions()\n    10\n    \n    Repartition the data into 7 partitions by 'age' column.\n    \n    &gt;&gt;&gt; df.repartition(7, \"age\").rdd.getNumPartitions()\n    7\n    \n    Repartition the data into 7 partitions by 'age' and 'name columns.\n    \n    &gt;&gt;&gt; df.repartition(3, \"name\", \"age\").rdd.getNumPartitions()\n    3\n\n\n\n\ndf = spark.createDataFrame([(1, \"Goku\"), (2, \"Naruto\")], schema=[\"id\", \"name\"])\n\ndf.show()\ndf.printSchema()\n\n+---+------+\n| id|  name|\n+---+------+\n|  1|  Goku|\n|  2|Naruto|\n+---+------+\n\nroot\n |-- id: long (nullable = true)\n |-- name: string (nullable = true)\n\n\n\n\ndf_1part = df.repartition(1)\ndf_1part.rdd.getNumPartitions()\n\n1\n\n\n\ndf_1part\\\n.write\\\n.csv(\"df_csv_1part\", mode=\"overwrite\", header=True)\n\n\nspark.read\\\n.option(\"header\", True)\\\n.format(\"csv\")\\\n.load(\"df_csv_1part\").show()\n\n+---+------+\n| id|  name|\n+---+------+\n|  1|  Goku|\n|  2|Naruto|\n+---+------+\n\n\n\n\n!ls -la df_csv_1part/\n\ntotal 20\ndrwxr-xr-x 2 pedro-wsl pedro-wsl 4096 Oct 25 15:13 .\ndrwxr-xr-x 6 pedro-wsl pedro-wsl 4096 Oct 25 15:13 ..\n-rw-r--r-- 1 pedro-wsl pedro-wsl    8 Oct 25 15:13 ._SUCCESS.crc\n-rw-r--r-- 1 pedro-wsl pedro-wsl   12 Oct 25 15:13 .part-00000-48e65ab0-a0d2-4a48-8f0c-e0b85439f207-c000.csv.crc\n-rw-r--r-- 1 pedro-wsl pedro-wsl    0 Oct 25 15:13 _SUCCESS\n-rw-r--r-- 1 pedro-wsl pedro-wsl   24 Oct 25 15:13 part-00000-48e65ab0-a0d2-4a48-8f0c-e0b85439f207-c000.csv"
  },
  {
    "objectID": "reading-json-files.html",
    "href": "reading-json-files.html",
    "title": "Reading JSON files",
    "section": "",
    "text": "Based on WafaStudies PySpark tutorial."
  },
  {
    "objectID": "reading-json-files.html#imports",
    "href": "reading-json-files.html#imports",
    "title": "Reading JSON files",
    "section": "Imports",
    "text": "Imports\n\nimport findspark\nfindspark.init()\n\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\nspark = SparkSession.builder\\\n                    .appName('Spark')\\\n                    .master(\"local[*]\")\\\n                    .getOrCreate()\n\nyour 131072x1 screen size is bogus. expect trouble\n23/10/25 15:13:10 WARN Utils: Your hostname, PC resolves to a loopback address: 127.0.1.1; using 172.29.148.244 instead (on interface eth0)\n23/10/25 15:13:10 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n23/10/25 15:13:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n23/10/25 15:13:12 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n23/10/25 15:13:12 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n23/10/25 15:13:12 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043."
  },
  {
    "objectID": "reading-json-files.html#generate-data",
    "href": "reading-json-files.html#generate-data",
    "title": "Reading JSON files",
    "section": "Generate data",
    "text": "Generate data\n\n!mkdir data\n!mkdir ml_data\n\nmkdir: cannot create directory ‘data’: File exists\n\n\n\nimport json\nimport random\nfrom faker import Faker\n\nfaker = Faker('pt_BR')\n\n# Generate data for employees1.json\nwith open('data/employees1.json', 'w') as jsonfile:\n    for id in range(1, 6):\n        name = faker.name()\n        salary = random.randint(1000, 10000)\n        employee = {'id': id, 'name': name, 'salary': float(salary)}\n        json.dump(employee, jsonfile)\n        jsonfile.write('\\n')\n\n# Generate data for employees2.json\nemployees2_data = []\nfor id in range(1, 6):\n    name = faker.name()\n    salary = random.randint(1000, 10000)\n    employee = {'id': id, 'name': name, 'salary': float(salary)}\n    employees2_data.append(employee)\n\nwith open('ml_data/employees2.json', 'w') as jsonfile:\n    json.dump(employees2_data, jsonfile, indent=2)\n\n# Generate data for employees3.json\nwith open('data/employees3.json', 'w') as jsonfile:\n    for id in range(6, 11):\n        name = faker.name()\n        salary = random.randint(1000, 10000)\n        employee = {'id': id, 'name': name, 'salary': float(salary)}\n        json.dump(employee, jsonfile)\n        jsonfile.write('\\n')"
  },
  {
    "objectID": "reading-json-files.html#reading-json-files",
    "href": "reading-json-files.html#reading-json-files",
    "title": "Reading JSON files",
    "section": "Reading json files",
    "text": "Reading json files\n\nhelp(spark.read.json)\n\nHelp on method json in module pyspark.sql.readwriter:\n\njson(path: Union[str, List[str], pyspark.rdd.RDD[str]], schema: Union[pyspark.sql.types.StructType, str, NoneType] = None, primitivesAsString: Union[bool, str, NoneType] = None, prefersDecimal: Union[bool, str, NoneType] = None, allowComments: Union[bool, str, NoneType] = None, allowUnquotedFieldNames: Union[bool, str, NoneType] = None, allowSingleQuotes: Union[bool, str, NoneType] = None, allowNumericLeadingZero: Union[bool, str, NoneType] = None, allowBackslashEscapingAnyCharacter: Union[bool, str, NoneType] = None, mode: Optional[str] = None, columnNameOfCorruptRecord: Optional[str] = None, dateFormat: Optional[str] = None, timestampFormat: Optional[str] = None, multiLine: Union[bool, str, NoneType] = None, allowUnquotedControlChars: Union[bool, str, NoneType] = None, lineSep: Optional[str] = None, samplingRatio: Union[str, float, NoneType] = None, dropFieldIfAllNull: Union[bool, str, NoneType] = None, encoding: Optional[str] = None, locale: Optional[str] = None, pathGlobFilter: Union[bool, str, NoneType] = None, recursiveFileLookup: Union[bool, str, NoneType] = None, modifiedBefore: Union[bool, str, NoneType] = None, modifiedAfter: Union[bool, str, NoneType] = None, allowNonNumericNumbers: Union[bool, str, NoneType] = None) -&gt; 'DataFrame' method of pyspark.sql.readwriter.DataFrameReader instance\n    Loads JSON files and returns the results as a :class:`DataFrame`.\n    \n    `JSON Lines &lt;http://jsonlines.org/&gt;`_ (newline-delimited JSON) is supported by default.\n    For JSON (one record per file), set the ``multiLine`` parameter to ``true``.\n    \n    If the ``schema`` parameter is not specified, this function goes\n    through the input once to determine the input schema.\n    \n    .. versionadded:: 1.4.0\n    \n    .. versionchanged:: 3.4.0\n        Supports Spark Connect.\n    \n    Parameters\n    ----------\n    path : str, list or :class:`RDD`\n        string represents path to the JSON dataset, or a list of paths,\n        or RDD of Strings storing JSON objects.\n    schema : :class:`pyspark.sql.types.StructType` or str, optional\n        an optional :class:`pyspark.sql.types.StructType` for the input schema or\n        a DDL-formatted string (For example ``col0 INT, col1 DOUBLE``).\n    \n    Other Parameters\n    ----------------\n    Extra options\n        For the extra options, refer to\n        `Data Source Option &lt;https://spark.apache.org/docs/latest/sql-data-sources-json.html#data-source-option&gt;`_\n        for the version you use.\n    \n        .. # noqa\n    \n    Examples\n    --------\n    Write a DataFrame into a JSON file and read it back.\n    \n    &gt;&gt;&gt; import tempfile\n    &gt;&gt;&gt; with tempfile.TemporaryDirectory() as d:\n    ...     # Write a DataFrame into a JSON file\n    ...     spark.createDataFrame(\n    ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]\n    ...     ).write.mode(\"overwrite\").format(\"json\").save(d)\n    ...\n    ...     # Read the JSON file as a DataFrame.\n    ...     spark.read.json(d).show()\n    +---+------------+\n    |age|        name|\n    +---+------------+\n    |100|Hyukjin Kwon|\n    +---+------------+\n\n\n\n\ndf = spark.read.json(\"data/employees1.json\")\ndf.printSchema()\ndf.show()\n\nroot\n |-- id: long (nullable = true)\n |-- name: string (nullable = true)\n |-- salary: double (nullable = true)\n\n+---+--------------------+------+\n| id|                name|salary|\n+---+--------------------+------+\n|  1|   Giovanna Teixeira|6238.0|\n|  2|Dra. Isabel Silveira|2311.0|\n|  3|   Davi Luiz da Rosa|5428.0|\n|  4|João Gabriel Cald...|8027.0|\n|  5|    Natália Teixeira|8968.0|\n+---+--------------------+------+\n\n\n\nWe can also tell spark the schema:\n\nschema = \"id long, name string, salary double\"\n\n\ndf = spark.read.json(\"data/employees1.json\", schema=schema)\ndf.printSchema()\ndf.show()\n\nroot\n |-- id: long (nullable = true)\n |-- name: string (nullable = true)\n |-- salary: double (nullable = true)\n\n+---+--------------------+------+\n| id|                name|salary|\n+---+--------------------+------+\n|  1|   Giovanna Teixeira|6238.0|\n|  2|Dra. Isabel Silveira|2311.0|\n|  3|   Davi Luiz da Rosa|5428.0|\n|  4|João Gabriel Cald...|8027.0|\n|  5|    Natália Teixeira|8968.0|\n+---+--------------------+------+\n\n\n\nWhat if the json file is multiline?\n\nwith open('ml_data/employees2.json', 'r') as file:\n    pretty_json = json.dumps(json.load(file), indent=4)\n    print(pretty_json)\n\n[\n    {\n        \"id\": 1,\n        \"name\": \"Pedro Henrique Moreira\",\n        \"salary\": 7979.0\n    },\n    {\n        \"id\": 2,\n        \"name\": \"Jo\\u00e3o Miguel da Concei\\u00e7\\u00e3o\",\n        \"salary\": 5590.0\n    },\n    {\n        \"id\": 3,\n        \"name\": \"Luiz Miguel Monteiro\",\n        \"salary\": 7854.0\n    },\n    {\n        \"id\": 4,\n        \"name\": \"Jo\\u00e3o Miguel Porto\",\n        \"salary\": 4581.0\n    },\n    {\n        \"id\": 5,\n        \"name\": \"Dr. Luiz Gustavo Moraes\",\n        \"salary\": 9965.0\n    }\n]\n\n\nLet’s try to read it:\n\ndf = spark.read.json(\"ml_data/employees2.json\")\ndf.printSchema()\ndf.show()\n\nroot\n |-- _corrupt_record: string (nullable = true)\n\n\n\nAnalysisException: Since Spark 2.3, the queries from raw JSON/CSV files are disallowed when the\nreferenced columns only include the internal corrupt record column\n(named _corrupt_record by default). For example:\nspark.read.schema(schema).csv(file).filter($\"_corrupt_record\".isNotNull).count()\nand spark.read.schema(schema).csv(file).select(\"_corrupt_record\").show().\nInstead, you can cache or save the parsed results and then send the same query.\nFor example, val df = spark.read.schema(schema).csv(file).cache() and then\ndf.filter($\"_corrupt_record\".isNotNull).count().\n\n\nWe got an error\nTo fix it, we need to tell spark the json file is multiline:\n\ndf = spark.read.json(\"ml_data/employees2.json\", multiLine=True)\ndf.printSchema()\ndf.show()\n\nroot\n |-- id: long (nullable = true)\n |-- name: string (nullable = true)\n |-- salary: double (nullable = true)\n\n+---+--------------------+------+\n| id|                name|salary|\n+---+--------------------+------+\n|  1|Pedro Henrique Mo...|7979.0|\n|  2|João Miguel da Co...|5590.0|\n|  3|Luiz Miguel Monteiro|7854.0|\n|  4|   João Miguel Porto|4581.0|\n|  5|Dr. Luiz Gustavo ...|9965.0|\n+---+--------------------+------+\n\n\n\nWhat if we want to load multiple json files?\nJust use a list :)\n\ndf = spark.read.json([\"data/employees1.json\", \"data/employees3.json\"])\ndf.printSchema()\ndf.show()\n\nroot\n |-- id: long (nullable = true)\n |-- name: string (nullable = true)\n |-- salary: double (nullable = true)\n\n+---+--------------------+------+\n| id|                name|salary|\n+---+--------------------+------+\n|  1|   Giovanna Teixeira|6238.0|\n|  2|Dra. Isabel Silveira|2311.0|\n|  3|   Davi Luiz da Rosa|5428.0|\n|  4|João Gabriel Cald...|8027.0|\n|  5|    Natália Teixeira|8968.0|\n|  6|     Marcela Freitas|8091.0|\n|  7|     Stephany Fogaça|3337.0|\n|  8|     Vicente Moreira|4439.0|\n|  9|       Leandro Cunha|2241.0|\n| 10|      Amanda Peixoto|5925.0|\n+---+--------------------+------+\n\n\n\nIf all the files are in the same folder, you can pass *.json:\n\ndf = spark.read.json(\"data/*.json\")\ndf.printSchema()\ndf.show()\n\nroot\n |-- id: long (nullable = true)\n |-- name: string (nullable = true)\n |-- salary: double (nullable = true)\n\n+---+--------------------+------+\n| id|                name|salary|\n+---+--------------------+------+\n|  1|   Giovanna Teixeira|6238.0|\n|  2|Dra. Isabel Silveira|2311.0|\n|  3|   Davi Luiz da Rosa|5428.0|\n|  4|João Gabriel Cald...|8027.0|\n|  5|    Natália Teixeira|8968.0|\n|  6|     Marcela Freitas|8091.0|\n|  7|     Stephany Fogaça|3337.0|\n|  8|     Vicente Moreira|4439.0|\n|  9|       Leandro Cunha|2241.0|\n| 10|      Amanda Peixoto|5925.0|\n+---+--------------------+------+"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Learning PySpark",
    "section": "",
    "text": "PySpark is a powerful open-source data processing framework that allows you to work with large datasets using Python. This project serves as a platform for me to explore and learn PySpark’s capabilities and features."
  }
]