{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNwY2ORtLgP6kSxLXoB8hn6"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Based on [WafaStudies](https://www.youtube.com/@WafaStudies) PySpark [tutorial](https://www.youtube.com/playlist?list=PLMWaZteqtEaJFiJ2FyIKK0YEuXwQ9YIS_)."
      ],
      "metadata": {
        "id": "CmlEeVvys-2Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "DnQUnkcLjqcA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz\n",
        "!tar xf spark-3.5.0-bin-hadoop3.tgz\n",
        "!pip -q install findspark"
      ],
      "metadata": {
        "id": "AMJqMeKMEby2"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.0-bin-hadoop3\""
      ],
      "metadata": {
        "id": "YCAEgkYiEdV2"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "hbpmyFRaEeyf"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "spark = SparkSession.builder\\\n",
        "                    .appName('Spark')\\\n",
        "                    .master(\"local[*]\")\\\n",
        "                    .getOrCreate()"
      ],
      "metadata": {
        "id": "J-69JCOeEgVZ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate data"
      ],
      "metadata": {
        "id": "G1cMSr_77dST"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faker"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfY3rPwv7am1",
        "outputId": "011949d9-ca83-41aa-89ac-d7cde8598656"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faker\n",
            "  Downloading Faker-19.6.2-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.4 in /usr/local/lib/python3.10/dist-packages (from faker) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.4->faker) (1.16.0)\n",
            "Installing collected packages: faker\n",
            "Successfully installed faker-19.6.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir data"
      ],
      "metadata": {
        "id": "HlCq4vdS9BUd"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import random\n",
        "from faker import Faker\n",
        "\n",
        "faker = Faker()\n",
        "\n",
        "with open('data/employees1.csv', 'w', newline='') as csvfile:\n",
        "    fieldnames = ['id', 'name', 'gender', 'salary']\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "    writer.writeheader()\n",
        "\n",
        "    for id in range(1, 6):\n",
        "        name = faker.name()\n",
        "        gender = random.choice(['Male', 'Female'])\n",
        "        salary = random.randint(1000, 10000)\n",
        "\n",
        "        writer.writerow({'id': id, 'name': name, 'gender': gender, 'salary': salary})\n",
        "\n",
        "with open('data/employees2.csv', 'w', newline='') as csvfile:\n",
        "    fieldnames = ['id', 'name', 'gender', 'salary']\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "\n",
        "    writer.writeheader()\n",
        "\n",
        "    for id in range(1, 6):\n",
        "        name = faker.name()\n",
        "        gender = random.choice(['Male', 'Female'])\n",
        "        salary = random.randint(1000, 10000)\n",
        "\n",
        "        writer.writerow({'id': id, 'name': name, 'gender': gender, 'salary': salary})"
      ],
      "metadata": {
        "id": "RQCXHeyy7hR1"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reading CSV files"
      ],
      "metadata": {
        "id": "dyCqzNtlw6DA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "help(spark.read.csv)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vl8Egoxqw8jP",
        "outputId": "1f00b2e8-5f3b-4be2-b4be-7b80ce043c82"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on method csv in module pyspark.sql.readwriter:\n",
            "\n",
            "csv(path: Union[str, List[str]], schema: Union[pyspark.sql.types.StructType, str, NoneType] = None, sep: Optional[str] = None, encoding: Optional[str] = None, quote: Optional[str] = None, escape: Optional[str] = None, comment: Optional[str] = None, header: Union[bool, str, NoneType] = None, inferSchema: Union[bool, str, NoneType] = None, ignoreLeadingWhiteSpace: Union[bool, str, NoneType] = None, ignoreTrailingWhiteSpace: Union[bool, str, NoneType] = None, nullValue: Optional[str] = None, nanValue: Optional[str] = None, positiveInf: Optional[str] = None, negativeInf: Optional[str] = None, dateFormat: Optional[str] = None, timestampFormat: Optional[str] = None, maxColumns: Union[str, int, NoneType] = None, maxCharsPerColumn: Union[str, int, NoneType] = None, maxMalformedLogPerPartition: Union[str, int, NoneType] = None, mode: Optional[str] = None, columnNameOfCorruptRecord: Optional[str] = None, multiLine: Union[bool, str, NoneType] = None, charToEscapeQuoteEscaping: Optional[str] = None, samplingRatio: Union[str, float, NoneType] = None, enforceSchema: Union[bool, str, NoneType] = None, emptyValue: Optional[str] = None, locale: Optional[str] = None, lineSep: Optional[str] = None, pathGlobFilter: Union[bool, str, NoneType] = None, recursiveFileLookup: Union[bool, str, NoneType] = None, modifiedBefore: Union[bool, str, NoneType] = None, modifiedAfter: Union[bool, str, NoneType] = None, unescapedQuoteHandling: Optional[str] = None) -> 'DataFrame' method of pyspark.sql.readwriter.DataFrameReader instance\n",
            "    Loads a CSV file and returns the result as a  :class:`DataFrame`.\n",
            "    \n",
            "    This function will go through the input once to determine the input schema if\n",
            "    ``inferSchema`` is enabled. To avoid going through the entire data once, disable\n",
            "    ``inferSchema`` option or specify the schema explicitly using ``schema``.\n",
            "    \n",
            "    .. versionadded:: 2.0.0\n",
            "    \n",
            "    .. versionchanged:: 3.4.0\n",
            "        Supports Spark Connect.\n",
            "    \n",
            "    Parameters\n",
            "    ----------\n",
            "    path : str or list\n",
            "        string, or list of strings, for input path(s),\n",
            "        or RDD of Strings storing CSV rows.\n",
            "    schema : :class:`pyspark.sql.types.StructType` or str, optional\n",
            "        an optional :class:`pyspark.sql.types.StructType` for the input schema\n",
            "        or a DDL-formatted string (For example ``col0 INT, col1 DOUBLE``).\n",
            "    \n",
            "    Other Parameters\n",
            "    ----------------\n",
            "    Extra options\n",
            "        For the extra options, refer to\n",
            "        `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-csv.html#data-source-option>`_\n",
            "        for the version you use.\n",
            "    \n",
            "        .. # noqa\n",
            "    \n",
            "    Examples\n",
            "    --------\n",
            "    Write a DataFrame into a CSV file and read it back.\n",
            "    \n",
            "    >>> import tempfile\n",
            "    >>> with tempfile.TemporaryDirectory() as d:\n",
            "    ...     # Write a DataFrame into a CSV file\n",
            "    ...     df = spark.createDataFrame([{\"age\": 100, \"name\": \"Hyukjin Kwon\"}])\n",
            "    ...     df.write.mode(\"overwrite\").format(\"csv\").save(d)\n",
            "    ...\n",
            "    ...     # Read the CSV file as a DataFrame with 'nullValue' option set to 'Hyukjin Kwon'.\n",
            "    ...     spark.read.csv(d, schema=df.schema, nullValue=\"Hyukjin Kwon\").show()\n",
            "    +---+----+\n",
            "    |age|name|\n",
            "    +---+----+\n",
            "    |100|NULL|\n",
            "    +---+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.csv(path='data/employees1.csv')\n",
        "df.show()\n",
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJQhNmld4P3b",
        "outputId": "fec3a8ee-9b26-4da3-c743-85cd2068d611"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------------+------+------+\n",
            "|_c0|          _c1|   _c2|   _c3|\n",
            "+---+-------------+------+------+\n",
            "| id|         name|gender|salary|\n",
            "|  1|  Holly Brown|  Male|  5513|\n",
            "|  2|Charles Baker|  Male|  8067|\n",
            "|  3|Regina Crosby|  Male|  4562|\n",
            "|  4|  Mark Flores|  Male|  6676|\n",
            "|  5|Daniel Snyder|  Male|  3155|\n",
            "+---+-------------+------+------+\n",
            "\n",
            "root\n",
            " |-- _c0: string (nullable = true)\n",
            " |-- _c1: string (nullable = true)\n",
            " |-- _c2: string (nullable = true)\n",
            " |-- _c3: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "By default, spark read csv without header and all datatypes as string.\n",
        "\n",
        "To avoid it, we use:\n",
        "\n",
        "```header=True```: first line will be taken as header\n",
        "\n",
        "```inferSchema=True```: spark will infer the datatypes of each column\n",
        "\n"
      ],
      "metadata": {
        "id": "JbYdK2fU_ftq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.csv(path='data/employees1.csv', header=True, inferSchema=True)\n",
        "df.show()\n",
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hhjn0yd-zw2",
        "outputId": "665196e0-6c72-47b4-ff21-f6e0f921af44"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------------+------+------+\n",
            "| id|         name|gender|salary|\n",
            "+---+-------------+------+------+\n",
            "|  1|  Holly Brown|  Male|  5513|\n",
            "|  2|Charles Baker|  Male|  8067|\n",
            "|  3|Regina Crosby|  Male|  4562|\n",
            "|  4|  Mark Flores|  Male|  6676|\n",
            "|  5|Daniel Snyder|  Male|  3155|\n",
            "+---+-------------+------+------+\n",
            "\n",
            "root\n",
            " |-- id: integer (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            " |-- salary: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```inferSchema``` takes some time and processing power, so we can tell spark the schema:"
      ],
      "metadata": {
        "id": "FjFamQtqAEl6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "schema = 'id integer, name string, gender string, salary double'"
      ],
      "metadata": {
        "id": "-WlUZJbjIDD9"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.csv(path='data/employees1.csv', header=True, schema=schema)\n",
        "\n",
        "df.show()\n",
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zCClo8ZH_Zrh",
        "outputId": "50347aa5-f0bd-4491-92c8-07cb6234e709"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------------+------+------+\n",
            "| id|         name|gender|salary|\n",
            "+---+-------------+------+------+\n",
            "|  1|  Holly Brown|  Male|5513.0|\n",
            "|  2|Charles Baker|  Male|8067.0|\n",
            "|  3|Regina Crosby|  Male|4562.0|\n",
            "|  4|  Mark Flores|  Male|6676.0|\n",
            "|  5|Daniel Snyder|  Male|3155.0|\n",
            "+---+-------------+------+------+\n",
            "\n",
            "root\n",
            " |-- id: integer (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            " |-- salary: double (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also read multiple files in one dataframe:"
      ],
      "metadata": {
        "id": "duSU9lZQHaCc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.csv(path=['data/employees1.csv', 'data/employees2.csv'], header=True, schema=schema)\n",
        "\n",
        "df.show()\n",
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3teG3Pr9G7rI",
        "outputId": "fd2642a9-05bc-48f0-e545-7387577df7fe"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------------+------+------+\n",
            "| id|          name|gender|salary|\n",
            "+---+--------------+------+------+\n",
            "|  1|Veronica Davis|  Male|3838.0|\n",
            "|  2|   Misty Young|Female|8519.0|\n",
            "|  3| David Sanchez|  Male|5335.0|\n",
            "|  4|Patricia Huber|Female|3183.0|\n",
            "|  5|    Ann Jensen|Female|8023.0|\n",
            "|  1|   Holly Brown|  Male|5513.0|\n",
            "|  2| Charles Baker|  Male|8067.0|\n",
            "|  3| Regina Crosby|  Male|4562.0|\n",
            "|  4|   Mark Flores|  Male|6676.0|\n",
            "|  5| Daniel Snyder|  Male|3155.0|\n",
            "+---+--------------+------+------+\n",
            "\n",
            "root\n",
            " |-- id: integer (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            " |-- salary: double (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If all the files are in the same folder, it's possible to use the folder path:"
      ],
      "metadata": {
        "id": "_T47maaUHox6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.csv(path=['data/'], header=True, schema=schema)\n",
        "\n",
        "df.show()\n",
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5ykYIFiHmYv",
        "outputId": "64ef3fbd-1c0d-40db-8b12-0bb32c333654"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------------+------+------+\n",
            "| id|          name|gender|salary|\n",
            "+---+--------------+------+------+\n",
            "|  1|Veronica Davis|  Male|3838.0|\n",
            "|  2|   Misty Young|Female|8519.0|\n",
            "|  3| David Sanchez|  Male|5335.0|\n",
            "|  4|Patricia Huber|Female|3183.0|\n",
            "|  5|    Ann Jensen|Female|8023.0|\n",
            "|  1|   Holly Brown|  Male|5513.0|\n",
            "|  2| Charles Baker|  Male|8067.0|\n",
            "|  3| Regina Crosby|  Male|4562.0|\n",
            "|  4|   Mark Flores|  Male|6676.0|\n",
            "|  5| Daniel Snyder|  Male|3155.0|\n",
            "+---+--------------+------+------+\n",
            "\n",
            "root\n",
            " |-- id: integer (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            " |-- salary: double (nullable = true)\n",
            "\n"
          ]
        }
      ]
    }
  ]
}