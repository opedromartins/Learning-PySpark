{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmlEeVvys-2Y"
      },
      "source": [
        "Based on [WafaStudies](https://www.youtube.com/@WafaStudies) PySpark [tutorial](https://www.youtube.com/playlist?list=PLMWaZteqtEaJFiJ2FyIKK0YEuXwQ9YIS_)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Reading CSV files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| default_exp Reading CSV files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| hide\n",
        "from nbdev.showdoc import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnQUnkcLjqcA"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "hbpmyFRaEeyf"
      },
      "outputs": [],
      "source": [
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "J-69JCOeEgVZ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "your 131072x1 screen size is bogus. expect trouble\n",
            "23/10/25 15:11:16 WARN Utils: Your hostname, PC resolves to a loopback address: 127.0.1.1; using 172.29.148.244 instead (on interface eth0)\n",
            "23/10/25 15:11:16 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
            "23/10/25 15:11:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "23/10/25 15:11:18 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "spark = SparkSession.builder\\\n",
        "                    .appName('Spark')\\\n",
        "                    .master(\"local[*]\")\\\n",
        "                    .getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1cMSr_77dST"
      },
      "source": [
        "## Generate data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "HlCq4vdS9BUd"
      },
      "outputs": [],
      "source": [
        "!mkdir data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "RQCXHeyy7hR1"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import random\n",
        "from faker import Faker\n",
        "\n",
        "faker = Faker()\n",
        "\n",
        "with open('data/employees1.csv', 'w', newline='') as csvfile:\n",
        "    fieldnames = ['id', 'name', 'gender', 'salary']\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "    writer.writeheader()\n",
        "\n",
        "    for id in range(1, 6):\n",
        "        name = faker.name()\n",
        "        gender = random.choice(['Male', 'Female'])\n",
        "        salary = random.randint(1000, 10000)\n",
        "\n",
        "        writer.writerow({'id': id, 'name': name, 'gender': gender, 'salary': salary})\n",
        "\n",
        "with open('data/employees2.csv', 'w', newline='') as csvfile:\n",
        "    fieldnames = ['id', 'name', 'gender', 'salary']\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "\n",
        "    writer.writeheader()\n",
        "\n",
        "    for id in range(1, 6):\n",
        "        name = faker.name()\n",
        "        gender = random.choice(['Male', 'Female'])\n",
        "        salary = random.randint(1000, 10000)\n",
        "\n",
        "        writer.writerow({'id': id, 'name': name, 'gender': gender, 'salary': salary})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyCqzNtlw6DA"
      },
      "source": [
        "## Reading CSV files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vl8Egoxqw8jP",
        "outputId": "1f00b2e8-5f3b-4be2-b4be-7b80ce043c82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Help on method csv in module pyspark.sql.readwriter:\n",
            "\n",
            "csv(path: Union[str, List[str]], schema: Union[pyspark.sql.types.StructType, str, NoneType] = None, sep: Optional[str] = None, encoding: Optional[str] = None, quote: Optional[str] = None, escape: Optional[str] = None, comment: Optional[str] = None, header: Union[bool, str, NoneType] = None, inferSchema: Union[bool, str, NoneType] = None, ignoreLeadingWhiteSpace: Union[bool, str, NoneType] = None, ignoreTrailingWhiteSpace: Union[bool, str, NoneType] = None, nullValue: Optional[str] = None, nanValue: Optional[str] = None, positiveInf: Optional[str] = None, negativeInf: Optional[str] = None, dateFormat: Optional[str] = None, timestampFormat: Optional[str] = None, maxColumns: Union[str, int, NoneType] = None, maxCharsPerColumn: Union[str, int, NoneType] = None, maxMalformedLogPerPartition: Union[str, int, NoneType] = None, mode: Optional[str] = None, columnNameOfCorruptRecord: Optional[str] = None, multiLine: Union[bool, str, NoneType] = None, charToEscapeQuoteEscaping: Optional[str] = None, samplingRatio: Union[str, float, NoneType] = None, enforceSchema: Union[bool, str, NoneType] = None, emptyValue: Optional[str] = None, locale: Optional[str] = None, lineSep: Optional[str] = None, pathGlobFilter: Union[bool, str, NoneType] = None, recursiveFileLookup: Union[bool, str, NoneType] = None, modifiedBefore: Union[bool, str, NoneType] = None, modifiedAfter: Union[bool, str, NoneType] = None, unescapedQuoteHandling: Optional[str] = None) -> 'DataFrame' method of pyspark.sql.readwriter.DataFrameReader instance\n",
            "    Loads a CSV file and returns the result as a  :class:`DataFrame`.\n",
            "    \n",
            "    This function will go through the input once to determine the input schema if\n",
            "    ``inferSchema`` is enabled. To avoid going through the entire data once, disable\n",
            "    ``inferSchema`` option or specify the schema explicitly using ``schema``.\n",
            "    \n",
            "    .. versionadded:: 2.0.0\n",
            "    \n",
            "    .. versionchanged:: 3.4.0\n",
            "        Supports Spark Connect.\n",
            "    \n",
            "    Parameters\n",
            "    ----------\n",
            "    path : str or list\n",
            "        string, or list of strings, for input path(s),\n",
            "        or RDD of Strings storing CSV rows.\n",
            "    schema : :class:`pyspark.sql.types.StructType` or str, optional\n",
            "        an optional :class:`pyspark.sql.types.StructType` for the input schema\n",
            "        or a DDL-formatted string (For example ``col0 INT, col1 DOUBLE``).\n",
            "    \n",
            "    Other Parameters\n",
            "    ----------------\n",
            "    Extra options\n",
            "        For the extra options, refer to\n",
            "        `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-csv.html#data-source-option>`_\n",
            "        for the version you use.\n",
            "    \n",
            "        .. # noqa\n",
            "    \n",
            "    Examples\n",
            "    --------\n",
            "    Write a DataFrame into a CSV file and read it back.\n",
            "    \n",
            "    >>> import tempfile\n",
            "    >>> with tempfile.TemporaryDirectory() as d:\n",
            "    ...     # Write a DataFrame into a CSV file\n",
            "    ...     df = spark.createDataFrame([{\"age\": 100, \"name\": \"Hyukjin Kwon\"}])\n",
            "    ...     df.write.mode(\"overwrite\").format(\"csv\").save(d)\n",
            "    ...\n",
            "    ...     # Read the CSV file as a DataFrame with 'nullValue' option set to 'Hyukjin Kwon'.\n",
            "    ...     spark.read.csv(d, schema=df.schema, nullValue=\"Hyukjin Kwon\").show()\n",
            "    +---+----+\n",
            "    |age|name|\n",
            "    +---+----+\n",
            "    |100|NULL|\n",
            "    +---+----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "help(spark.read.csv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJQhNmld4P3b",
        "outputId": "fec3a8ee-9b26-4da3-c743-85cd2068d611"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+----------------+------+------+\n",
            "|_c0|             _c1|   _c2|   _c3|\n",
            "+---+----------------+------+------+\n",
            "| id|            name|gender|salary|\n",
            "|  1|      Kara Moyer|  Male|  3265|\n",
            "|  2|    Kathryn Bell|  Male|  4657|\n",
            "|  3|   Gerald Newman|  Male|  4000|\n",
            "|  4|Angela Rodriguez|Female|  2596|\n",
            "|  5|     Terry Smith|Female|  3685|\n",
            "+---+----------------+------+------+\n",
            "\n",
            "root\n",
            " |-- _c0: string (nullable = true)\n",
            " |-- _c1: string (nullable = true)\n",
            " |-- _c2: string (nullable = true)\n",
            " |-- _c3: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df = spark.read.csv(path='data/employees1.csv')\n",
        "df.show()\n",
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbYdK2fU_ftq"
      },
      "source": [
        "By default, spark read csv without header and all datatypes as string.\n",
        "\n",
        "To avoid it, we use:\n",
        "\n",
        "```header=True```: first line will be taken as header\n",
        "\n",
        "```inferSchema=True```: spark will infer the datatypes of each column\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hhjn0yd-zw2",
        "outputId": "665196e0-6c72-47b4-ff21-f6e0f921af44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+----------------+------+------+\n",
            "| id|            name|gender|salary|\n",
            "+---+----------------+------+------+\n",
            "|  1|      Kara Moyer|  Male|  3265|\n",
            "|  2|    Kathryn Bell|  Male|  4657|\n",
            "|  3|   Gerald Newman|  Male|  4000|\n",
            "|  4|Angela Rodriguez|Female|  2596|\n",
            "|  5|     Terry Smith|Female|  3685|\n",
            "+---+----------------+------+------+\n",
            "\n",
            "root\n",
            " |-- id: integer (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            " |-- salary: integer (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df = spark.read.csv(path='data/employees1.csv', header=True, inferSchema=True)\n",
        "df.show()\n",
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjFamQtqAEl6"
      },
      "source": [
        "```inferSchema``` takes some time and processing power, so we can tell spark the schema:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "-WlUZJbjIDD9"
      },
      "outputs": [],
      "source": [
        "schema = 'id integer, name string, gender string, salary double'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zCClo8ZH_Zrh",
        "outputId": "50347aa5-f0bd-4491-92c8-07cb6234e709"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+----------------+------+------+\n",
            "| id|            name|gender|salary|\n",
            "+---+----------------+------+------+\n",
            "|  1|      Kara Moyer|  Male|3265.0|\n",
            "|  2|    Kathryn Bell|  Male|4657.0|\n",
            "|  3|   Gerald Newman|  Male|4000.0|\n",
            "|  4|Angela Rodriguez|Female|2596.0|\n",
            "|  5|     Terry Smith|Female|3685.0|\n",
            "+---+----------------+------+------+\n",
            "\n",
            "root\n",
            " |-- id: integer (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            " |-- salary: double (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df = spark.read.csv(path='data/employees1.csv', header=True, schema=schema)\n",
        "\n",
        "df.show()\n",
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duSU9lZQHaCc"
      },
      "source": [
        "We can also read multiple files in one dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3teG3Pr9G7rI",
        "outputId": "fd2642a9-05bc-48f0-e545-7387577df7fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+----------------+------+------+\n",
            "| id|            name|gender|salary|\n",
            "+---+----------------+------+------+\n",
            "|  1|   Brandon Davis|  Male|1786.0|\n",
            "|  2|   Amanda Hansen|  Male|9218.0|\n",
            "|  3|Valerie Peterson|Female|4119.0|\n",
            "|  4|    Robert Mason|  Male|7547.0|\n",
            "|  5|    James Thomas|Female|4181.0|\n",
            "|  1|      Kara Moyer|  Male|3265.0|\n",
            "|  2|    Kathryn Bell|  Male|4657.0|\n",
            "|  3|   Gerald Newman|  Male|4000.0|\n",
            "|  4|Angela Rodriguez|Female|2596.0|\n",
            "|  5|     Terry Smith|Female|3685.0|\n",
            "+---+----------------+------+------+\n",
            "\n",
            "root\n",
            " |-- id: integer (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            " |-- salary: double (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df = spark.read.csv(path=['data/employees1.csv', 'data/employees2.csv'], header=True, schema=schema)\n",
        "\n",
        "df.show()\n",
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_T47maaUHox6"
      },
      "source": [
        "If all the files are in the same folder, it's possible to use the folder path:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5ykYIFiHmYv",
        "outputId": "64ef3fbd-1c0d-40db-8b12-0bb32c333654"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+----------------+------+------+\n",
            "| id|            name|gender|salary|\n",
            "+---+----------------+------+------+\n",
            "|  1|   Brandon Davis|  Male|1786.0|\n",
            "|  2|   Amanda Hansen|  Male|9218.0|\n",
            "|  3|Valerie Peterson|Female|4119.0|\n",
            "|  4|    Robert Mason|  Male|7547.0|\n",
            "|  5|    James Thomas|Female|4181.0|\n",
            "|  1|      Kara Moyer|  Male|3265.0|\n",
            "|  2|    Kathryn Bell|  Male|4657.0|\n",
            "|  3|   Gerald Newman|  Male|4000.0|\n",
            "|  4|Angela Rodriguez|Female|2596.0|\n",
            "|  5|     Terry Smith|Female|3685.0|\n",
            "+---+----------------+------+------+\n",
            "\n",
            "root\n",
            " |-- id: integer (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            " |-- salary: double (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df = spark.read.csv(path=['data/'], header=True, schema=schema)\n",
        "\n",
        "df.show()\n",
        "df.printSchema()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNwY2ORtLgP6kSxLXoB8hn6",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
