{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmlEeVvys-2Y"
      },
      "source": [
        "Based on [WafaStudies](https://www.youtube.com/@WafaStudies) PySpark [tutorial](https://www.youtube.com/playlist?list=PLMWaZteqtEaJFiJ2FyIKK0YEuXwQ9YIS_)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Reading JSON files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| default_exp Reading JSON files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| hide\n",
        "from nbdev.showdoc import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnQUnkcLjqcA"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "hbpmyFRaEeyf"
      },
      "outputs": [],
      "source": [
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "J-69JCOeEgVZ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "your 131072x1 screen size is bogus. expect trouble\n",
            "23/10/25 15:13:10 WARN Utils: Your hostname, PC resolves to a loopback address: 127.0.1.1; using 172.29.148.244 instead (on interface eth0)\n",
            "23/10/25 15:13:10 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
            "23/10/25 15:13:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "23/10/25 15:13:12 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
            "23/10/25 15:13:12 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
            "23/10/25 15:13:12 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "spark = SparkSession.builder\\\n",
        "                    .appName('Spark')\\\n",
        "                    .master(\"local[*]\")\\\n",
        "                    .getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1cMSr_77dST"
      },
      "source": [
        "## Generate data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "HlCq4vdS9BUd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘data’: File exists\n"
          ]
        }
      ],
      "source": [
        "!mkdir data\n",
        "!mkdir ml_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "RQCXHeyy7hR1"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import random\n",
        "from faker import Faker\n",
        "\n",
        "faker = Faker('pt_BR')\n",
        "\n",
        "# Generate data for employees1.json\n",
        "with open('data/employees1.json', 'w') as jsonfile:\n",
        "    for id in range(1, 6):\n",
        "        name = faker.name()\n",
        "        salary = random.randint(1000, 10000)\n",
        "        employee = {'id': id, 'name': name, 'salary': float(salary)}\n",
        "        json.dump(employee, jsonfile)\n",
        "        jsonfile.write('\\n')\n",
        "\n",
        "# Generate data for employees2.json\n",
        "employees2_data = []\n",
        "for id in range(1, 6):\n",
        "    name = faker.name()\n",
        "    salary = random.randint(1000, 10000)\n",
        "    employee = {'id': id, 'name': name, 'salary': float(salary)}\n",
        "    employees2_data.append(employee)\n",
        "\n",
        "with open('ml_data/employees2.json', 'w') as jsonfile:\n",
        "    json.dump(employees2_data, jsonfile, indent=2)\n",
        "\n",
        "# Generate data for employees3.json\n",
        "with open('data/employees3.json', 'w') as jsonfile:\n",
        "    for id in range(6, 11):\n",
        "        name = faker.name()\n",
        "        salary = random.randint(1000, 10000)\n",
        "        employee = {'id': id, 'name': name, 'salary': float(salary)}\n",
        "        json.dump(employee, jsonfile)\n",
        "        jsonfile.write('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9lNQOajz_hy"
      },
      "source": [
        "## Reading json files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G36XnXXH0Bep",
        "outputId": "14366c6d-b13e-4103-9c47-a199708d447b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Help on method json in module pyspark.sql.readwriter:\n",
            "\n",
            "json(path: Union[str, List[str], pyspark.rdd.RDD[str]], schema: Union[pyspark.sql.types.StructType, str, NoneType] = None, primitivesAsString: Union[bool, str, NoneType] = None, prefersDecimal: Union[bool, str, NoneType] = None, allowComments: Union[bool, str, NoneType] = None, allowUnquotedFieldNames: Union[bool, str, NoneType] = None, allowSingleQuotes: Union[bool, str, NoneType] = None, allowNumericLeadingZero: Union[bool, str, NoneType] = None, allowBackslashEscapingAnyCharacter: Union[bool, str, NoneType] = None, mode: Optional[str] = None, columnNameOfCorruptRecord: Optional[str] = None, dateFormat: Optional[str] = None, timestampFormat: Optional[str] = None, multiLine: Union[bool, str, NoneType] = None, allowUnquotedControlChars: Union[bool, str, NoneType] = None, lineSep: Optional[str] = None, samplingRatio: Union[str, float, NoneType] = None, dropFieldIfAllNull: Union[bool, str, NoneType] = None, encoding: Optional[str] = None, locale: Optional[str] = None, pathGlobFilter: Union[bool, str, NoneType] = None, recursiveFileLookup: Union[bool, str, NoneType] = None, modifiedBefore: Union[bool, str, NoneType] = None, modifiedAfter: Union[bool, str, NoneType] = None, allowNonNumericNumbers: Union[bool, str, NoneType] = None) -> 'DataFrame' method of pyspark.sql.readwriter.DataFrameReader instance\n",
            "    Loads JSON files and returns the results as a :class:`DataFrame`.\n",
            "    \n",
            "    `JSON Lines <http://jsonlines.org/>`_ (newline-delimited JSON) is supported by default.\n",
            "    For JSON (one record per file), set the ``multiLine`` parameter to ``true``.\n",
            "    \n",
            "    If the ``schema`` parameter is not specified, this function goes\n",
            "    through the input once to determine the input schema.\n",
            "    \n",
            "    .. versionadded:: 1.4.0\n",
            "    \n",
            "    .. versionchanged:: 3.4.0\n",
            "        Supports Spark Connect.\n",
            "    \n",
            "    Parameters\n",
            "    ----------\n",
            "    path : str, list or :class:`RDD`\n",
            "        string represents path to the JSON dataset, or a list of paths,\n",
            "        or RDD of Strings storing JSON objects.\n",
            "    schema : :class:`pyspark.sql.types.StructType` or str, optional\n",
            "        an optional :class:`pyspark.sql.types.StructType` for the input schema or\n",
            "        a DDL-formatted string (For example ``col0 INT, col1 DOUBLE``).\n",
            "    \n",
            "    Other Parameters\n",
            "    ----------------\n",
            "    Extra options\n",
            "        For the extra options, refer to\n",
            "        `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-json.html#data-source-option>`_\n",
            "        for the version you use.\n",
            "    \n",
            "        .. # noqa\n",
            "    \n",
            "    Examples\n",
            "    --------\n",
            "    Write a DataFrame into a JSON file and read it back.\n",
            "    \n",
            "    >>> import tempfile\n",
            "    >>> with tempfile.TemporaryDirectory() as d:\n",
            "    ...     # Write a DataFrame into a JSON file\n",
            "    ...     spark.createDataFrame(\n",
            "    ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]\n",
            "    ...     ).write.mode(\"overwrite\").format(\"json\").save(d)\n",
            "    ...\n",
            "    ...     # Read the JSON file as a DataFrame.\n",
            "    ...     spark.read.json(d).show()\n",
            "    +---+------------+\n",
            "    |age|        name|\n",
            "    +---+------------+\n",
            "    |100|Hyukjin Kwon|\n",
            "    +---+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "help(spark.read.json)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48kFm-Bc0CUq",
        "outputId": "5e91e697-e332-4cfa-ddf2-db59a5271dfa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- salary: double (nullable = true)\n",
            "\n",
            "+---+--------------------+------+\n",
            "| id|                name|salary|\n",
            "+---+--------------------+------+\n",
            "|  1|   Giovanna Teixeira|6238.0|\n",
            "|  2|Dra. Isabel Silveira|2311.0|\n",
            "|  3|   Davi Luiz da Rosa|5428.0|\n",
            "|  4|João Gabriel Cald...|8027.0|\n",
            "|  5|    Natália Teixeira|8968.0|\n",
            "+---+--------------------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df = spark.read.json(\"data/employees1.json\")\n",
        "df.printSchema()\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkqByZCK8HUr"
      },
      "source": [
        "We can also tell spark the schema:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "MzcnUpy-9sMb"
      },
      "outputs": [],
      "source": [
        "schema = \"id long, name string, salary double\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eE62M-GP91Dx",
        "outputId": "7aba8529-89e5-40bc-885d-a354aa886b79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- salary: double (nullable = true)\n",
            "\n",
            "+---+--------------------+------+\n",
            "| id|                name|salary|\n",
            "+---+--------------------+------+\n",
            "|  1|   Giovanna Teixeira|6238.0|\n",
            "|  2|Dra. Isabel Silveira|2311.0|\n",
            "|  3|   Davi Luiz da Rosa|5428.0|\n",
            "|  4|João Gabriel Cald...|8027.0|\n",
            "|  5|    Natália Teixeira|8968.0|\n",
            "+---+--------------------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df = spark.read.json(\"data/employees1.json\", schema=schema)\n",
        "df.printSchema()\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C16kTdd7-pjf"
      },
      "source": [
        "What if the json file is multiline?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MEG0P_Y9_15g",
        "outputId": "ad9ecf77-a4a6-4798-a525-764f50e8a9f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[\n",
            "    {\n",
            "        \"id\": 1,\n",
            "        \"name\": \"Pedro Henrique Moreira\",\n",
            "        \"salary\": 7979.0\n",
            "    },\n",
            "    {\n",
            "        \"id\": 2,\n",
            "        \"name\": \"Jo\\u00e3o Miguel da Concei\\u00e7\\u00e3o\",\n",
            "        \"salary\": 5590.0\n",
            "    },\n",
            "    {\n",
            "        \"id\": 3,\n",
            "        \"name\": \"Luiz Miguel Monteiro\",\n",
            "        \"salary\": 7854.0\n",
            "    },\n",
            "    {\n",
            "        \"id\": 4,\n",
            "        \"name\": \"Jo\\u00e3o Miguel Porto\",\n",
            "        \"salary\": 4581.0\n",
            "    },\n",
            "    {\n",
            "        \"id\": 5,\n",
            "        \"name\": \"Dr. Luiz Gustavo Moraes\",\n",
            "        \"salary\": 9965.0\n",
            "    }\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "with open('ml_data/employees2.json', 'r') as file:\n",
        "    pretty_json = json.dumps(json.load(file), indent=4)\n",
        "    print(pretty_json)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTGmkYuKAhiz"
      },
      "source": [
        "Let's try to read it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529
        },
        "id": "0XfzB149-xIW",
        "outputId": "272b9835-b6af-47ac-f43e-d2d7d32206ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- _corrupt_record: string (nullable = true)\n",
            "\n"
          ]
        },
        {
          "ename": "AnalysisException",
          "evalue": "Since Spark 2.3, the queries from raw JSON/CSV files are disallowed when the\nreferenced columns only include the internal corrupt record column\n(named _corrupt_record by default). For example:\nspark.read.schema(schema).csv(file).filter($\"_corrupt_record\".isNotNull).count()\nand spark.read.schema(schema).csv(file).select(\"_corrupt_record\").show().\nInstead, you can cache or save the parsed results and then send the same query.\nFor example, val df = spark.read.schema(schema).csv(file).cache() and then\ndf.filter($\"_corrupt_record\".isNotNull).count().",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/home/pedro-wsl/Learning-PySpark/nbs/03_Reading-json-files.ipynb Cell 20\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pedro-wsl/Learning-PySpark/nbs/03_Reading-json-files.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m df \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39mread\u001b[39m.\u001b[39mjson(\u001b[39m\"\u001b[39m\u001b[39mml_data/employees2.json\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pedro-wsl/Learning-PySpark/nbs/03_Reading-json-files.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m df\u001b[39m.\u001b[39mprintSchema()\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pedro-wsl/Learning-PySpark/nbs/03_Reading-json-files.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m df\u001b[39m.\u001b[39;49mshow()\n",
            "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py:959\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    953\u001b[0m     \u001b[39mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    954\u001b[0m         error_class\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNOT_BOOL\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    955\u001b[0m         message_parameters\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39marg_name\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mvertical\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39marg_type\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mtype\u001b[39m(vertical)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m},\n\u001b[1;32m    956\u001b[0m     )\n\u001b[1;32m    958\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(truncate, \u001b[39mbool\u001b[39m) \u001b[39mand\u001b[39;00m truncate:\n\u001b[0;32m--> 959\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jdf\u001b[39m.\u001b[39;49mshowString(n, \u001b[39m20\u001b[39;49m, vertical))\n\u001b[1;32m    960\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    961\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
            "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
            "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: Since Spark 2.3, the queries from raw JSON/CSV files are disallowed when the\nreferenced columns only include the internal corrupt record column\n(named _corrupt_record by default). For example:\nspark.read.schema(schema).csv(file).filter($\"_corrupt_record\".isNotNull).count()\nand spark.read.schema(schema).csv(file).select(\"_corrupt_record\").show().\nInstead, you can cache or save the parsed results and then send the same query.\nFor example, val df = spark.read.schema(schema).csv(file).cache() and then\ndf.filter($\"_corrupt_record\".isNotNull).count()."
          ]
        }
      ],
      "source": [
        "df = spark.read.json(\"ml_data/employees2.json\")\n",
        "df.printSchema()\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iLUz9pG_D4o"
      },
      "source": [
        "We got an error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COBd0cCy-0uA"
      },
      "source": [
        "To fix it, we need to tell spark the json file is multiline:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9j-gJIGr_KX3",
        "outputId": "978e7fad-13be-4b4e-9f2f-5598e3f40677"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- salary: double (nullable = true)\n",
            "\n",
            "+---+--------------------+------+\n",
            "| id|                name|salary|\n",
            "+---+--------------------+------+\n",
            "|  1|Pedro Henrique Mo...|7979.0|\n",
            "|  2|João Miguel da Co...|5590.0|\n",
            "|  3|Luiz Miguel Monteiro|7854.0|\n",
            "|  4|   João Miguel Porto|4581.0|\n",
            "|  5|Dr. Luiz Gustavo ...|9965.0|\n",
            "+---+--------------------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df = spark.read.json(\"ml_data/employees2.json\", multiLine=True)\n",
        "df.printSchema()\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQqsqFGZBjGp"
      },
      "source": [
        "What if we want to load multiple json files?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVjJqes8N5Rd"
      },
      "source": [
        "Just use a list :)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GjnNYJKbAsNA",
        "outputId": "03ced1cf-41c6-4dc2-9b36-5d095463bfca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- salary: double (nullable = true)\n",
            "\n",
            "+---+--------------------+------+\n",
            "| id|                name|salary|\n",
            "+---+--------------------+------+\n",
            "|  1|   Giovanna Teixeira|6238.0|\n",
            "|  2|Dra. Isabel Silveira|2311.0|\n",
            "|  3|   Davi Luiz da Rosa|5428.0|\n",
            "|  4|João Gabriel Cald...|8027.0|\n",
            "|  5|    Natália Teixeira|8968.0|\n",
            "|  6|     Marcela Freitas|8091.0|\n",
            "|  7|     Stephany Fogaça|3337.0|\n",
            "|  8|     Vicente Moreira|4439.0|\n",
            "|  9|       Leandro Cunha|2241.0|\n",
            "| 10|      Amanda Peixoto|5925.0|\n",
            "+---+--------------------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df = spark.read.json([\"data/employees1.json\", \"data/employees3.json\"])\n",
        "df.printSchema()\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HyIC6lKOhwD"
      },
      "source": [
        "If all the files are in the same folder, you can pass ```*.json```:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52FEoWQtBwJR",
        "outputId": "f4a4f2b2-b68f-4e29-c6ee-6fb36cef5bbf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- salary: double (nullable = true)\n",
            "\n",
            "+---+--------------------+------+\n",
            "| id|                name|salary|\n",
            "+---+--------------------+------+\n",
            "|  1|   Giovanna Teixeira|6238.0|\n",
            "|  2|Dra. Isabel Silveira|2311.0|\n",
            "|  3|   Davi Luiz da Rosa|5428.0|\n",
            "|  4|João Gabriel Cald...|8027.0|\n",
            "|  5|    Natália Teixeira|8968.0|\n",
            "|  6|     Marcela Freitas|8091.0|\n",
            "|  7|     Stephany Fogaça|3337.0|\n",
            "|  8|     Vicente Moreira|4439.0|\n",
            "|  9|       Leandro Cunha|2241.0|\n",
            "| 10|      Amanda Peixoto|5925.0|\n",
            "+---+--------------------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df = spark.read.json(\"data/*.json\")\n",
        "df.printSchema()\n",
        "df.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyN87SuMb4aUwftLoFMQUbjw",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
