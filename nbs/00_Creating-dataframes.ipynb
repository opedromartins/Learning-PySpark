{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmlEeVvys-2Y"
      },
      "source": [
        "Based on [WafaStudies](https://www.youtube.com/@WafaStudies) PySpark [tutorial](https://www.youtube.com/playlist?list=PLMWaZteqtEaJFiJ2FyIKK0YEuXwQ9YIS_)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Creating Dataframes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| default_exp Creating Dataframes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| hide\n",
        "from nbdev.showdoc import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnQUnkcLjqcA"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "hbpmyFRaEeyf"
      },
      "outputs": [],
      "source": [
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "J-69JCOeEgVZ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "your 131072x1 screen size is bogus. expect trouble\n",
            "23/10/25 15:10:15 WARN Utils: Your hostname, PC resolves to a loopback address: 127.0.1.1; using 172.29.148.244 instead (on interface eth0)\n",
            "23/10/25 15:10:15 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
            "23/10/25 15:10:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "spark = SparkSession.builder\\\n",
        "                    .appName('Spark')\\\n",
        "                    .master(\"local[*]\")\\\n",
        "                    .getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfXkd_ktlHzV"
      },
      "source": [
        "## Create Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9ofx5DljpZK",
        "outputId": "59bc306f-63bc-4df5-95fa-6646d1298f6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Help on method createDataFrame in module pyspark.sql.session:\n",
            "\n",
            "createDataFrame(data: Union[pyspark.rdd.RDD[Any], Iterable[Any], ForwardRef('PandasDataFrameLike'), ForwardRef('ArrayLike')], schema: Union[pyspark.sql.types.AtomicType, pyspark.sql.types.StructType, str, NoneType] = None, samplingRatio: Optional[float] = None, verifySchema: bool = True) -> pyspark.sql.dataframe.DataFrame method of pyspark.sql.session.SparkSession instance\n",
            "    Creates a :class:`DataFrame` from an :class:`RDD`, a list, a :class:`pandas.DataFrame`\n",
            "    or a :class:`numpy.ndarray`.\n",
            "    \n",
            "    .. versionadded:: 2.0.0\n",
            "    \n",
            "    .. versionchanged:: 3.4.0\n",
            "        Supports Spark Connect.\n",
            "    \n",
            "    Parameters\n",
            "    ----------\n",
            "    data : :class:`RDD` or iterable\n",
            "        an RDD of any kind of SQL data representation (:class:`Row`,\n",
            "        :class:`tuple`, ``int``, ``boolean``, etc.), or :class:`list`,\n",
            "        :class:`pandas.DataFrame` or :class:`numpy.ndarray`.\n",
            "    schema : :class:`pyspark.sql.types.DataType`, str or list, optional\n",
            "        a :class:`pyspark.sql.types.DataType` or a datatype string or a list of\n",
            "        column names, default is None. The data type string format equals to\n",
            "        :class:`pyspark.sql.types.DataType.simpleString`, except that top level struct type can\n",
            "        omit the ``struct<>``.\n",
            "    \n",
            "        When ``schema`` is a list of column names, the type of each column\n",
            "        will be inferred from ``data``.\n",
            "    \n",
            "        When ``schema`` is ``None``, it will try to infer the schema (column names and types)\n",
            "        from ``data``, which should be an RDD of either :class:`Row`,\n",
            "        :class:`namedtuple`, or :class:`dict`.\n",
            "    \n",
            "        When ``schema`` is :class:`pyspark.sql.types.DataType` or a datatype string, it must\n",
            "        match the real data, or an exception will be thrown at runtime. If the given schema is\n",
            "        not :class:`pyspark.sql.types.StructType`, it will be wrapped into a\n",
            "        :class:`pyspark.sql.types.StructType` as its only field, and the field name will be\n",
            "        \"value\". Each record will also be wrapped into a tuple, which can be converted to row\n",
            "        later.\n",
            "    samplingRatio : float, optional\n",
            "        the sample ratio of rows used for inferring. The first few rows will be used\n",
            "        if ``samplingRatio`` is ``None``.\n",
            "    verifySchema : bool, optional\n",
            "        verify data types of every row against schema. Enabled by default.\n",
            "    \n",
            "        .. versionadded:: 2.1.0\n",
            "    \n",
            "    Returns\n",
            "    -------\n",
            "    :class:`DataFrame`\n",
            "    \n",
            "    Notes\n",
            "    -----\n",
            "    Usage with `spark.sql.execution.arrow.pyspark.enabled=True` is experimental.\n",
            "    \n",
            "    Examples\n",
            "    --------\n",
            "    Create a DataFrame from a list of tuples.\n",
            "    \n",
            "    >>> spark.createDataFrame([('Alice', 1)]).show()\n",
            "    +-----+---+\n",
            "    |   _1| _2|\n",
            "    +-----+---+\n",
            "    |Alice|  1|\n",
            "    +-----+---+\n",
            "    \n",
            "    Create a DataFrame from a list of dictionaries.\n",
            "    \n",
            "    >>> d = [{'name': 'Alice', 'age': 1}]\n",
            "    >>> spark.createDataFrame(d).show()\n",
            "    +---+-----+\n",
            "    |age| name|\n",
            "    +---+-----+\n",
            "    |  1|Alice|\n",
            "    +---+-----+\n",
            "    \n",
            "    Create a DataFrame with column names specified.\n",
            "    \n",
            "    >>> spark.createDataFrame([('Alice', 1)], ['name', 'age']).show()\n",
            "    +-----+---+\n",
            "    | name|age|\n",
            "    +-----+---+\n",
            "    |Alice|  1|\n",
            "    +-----+---+\n",
            "    \n",
            "    Create a DataFrame with the explicit schema specified.\n",
            "    \n",
            "    >>> from pyspark.sql.types import *\n",
            "    >>> schema = StructType([\n",
            "    ...    StructField(\"name\", StringType(), True),\n",
            "    ...    StructField(\"age\", IntegerType(), True)])\n",
            "    >>> spark.createDataFrame([('Alice', 1)], schema).show()\n",
            "    +-----+---+\n",
            "    | name|age|\n",
            "    +-----+---+\n",
            "    |Alice|  1|\n",
            "    +-----+---+\n",
            "    \n",
            "    Create a DataFrame with the schema in DDL formatted string.\n",
            "    \n",
            "    >>> spark.createDataFrame([('Alice', 1)], \"name: string, age: int\").show()\n",
            "    +-----+---+\n",
            "    | name|age|\n",
            "    +-----+---+\n",
            "    |Alice|  1|\n",
            "    +-----+---+\n",
            "    \n",
            "    Create an empty DataFrame.\n",
            "    When initializing an empty DataFrame in PySpark, it's mandatory to specify its schema,\n",
            "    as the DataFrame lacks data from which the schema can be inferred.\n",
            "    \n",
            "    >>> spark.createDataFrame([], \"name: string, age: int\").show()\n",
            "    +----+---+\n",
            "    |name|age|\n",
            "    +----+---+\n",
            "    +----+---+\n",
            "    \n",
            "    Create a DataFrame from Row objects.\n",
            "    \n",
            "    >>> from pyspark.sql import Row\n",
            "    >>> Person = Row('name', 'age')\n",
            "    >>> df = spark.createDataFrame([Person(\"Alice\", 1)])\n",
            "    >>> df.show()\n",
            "    +-----+---+\n",
            "    | name|age|\n",
            "    +-----+---+\n",
            "    |Alice|  1|\n",
            "    +-----+---+\n",
            "    \n",
            "    Create a DataFrame from a pandas DataFrame.\n",
            "    \n",
            "    >>> spark.createDataFrame(df.toPandas()).show()  # doctest: +SKIP\n",
            "    +-----+---+\n",
            "    | name|age|\n",
            "    +-----+---+\n",
            "    |Alice|  1|\n",
            "    +-----+---+\n",
            "    >>> spark.createDataFrame(pandas.DataFrame([[1, 2]])).collect()  # doctest: +SKIP\n",
            "    +---+---+\n",
            "    |  0|  1|\n",
            "    +---+---+\n",
            "    |  1|  2|\n",
            "    +---+---+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "help(spark.createDataFrame)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9m5wAq5UlUhs",
        "outputId": "b7da8a56-c4c4-4998-ec83-ca09886e9ecc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+---------+\n",
            "| _1|       _2|\n",
            "+---+---------+\n",
            "|  1|    Pedro|\n",
            "|  2|Guilherme|\n",
            "+---+---------+\n",
            "\n",
            "root\n",
            " |-- _1: long (nullable = true)\n",
            " |-- _2: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "data = [(1,'Pedro'), (2, 'Guilherme')]\n",
        "df = spark.createDataFrame(data=data)\n",
        "\n",
        "df.show()\n",
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dW0zhwMmS5o",
        "outputId": "1d5cbe04-b53d-4a85-ee20-847cddeec812"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+---------+\n",
            "| id|     name|\n",
            "+---+---------+\n",
            "|  1|    Pedro|\n",
            "|  2|Guilherme|\n",
            "+---+---------+\n",
            "\n",
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "data = [(1,'Pedro'), (2, 'Guilherme')]\n",
        "df = spark.createDataFrame(data=data, schema=[\"id\", \"name\"])\n",
        "\n",
        "df.show()\n",
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r6q7Qa2nq0YN",
        "outputId": "56978760-f57e-4e4a-ec8c-b52b56db033d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Help on class StructType in module pyspark.sql.types:\n",
            "\n",
            "class StructType(DataType)\n",
            " |  StructType(fields: Optional[List[pyspark.sql.types.StructField]] = None)\n",
            " |  \n",
            " |  Struct type, consisting of a list of :class:`StructField`.\n",
            " |  \n",
            " |  This is the data type representing a :class:`Row`.\n",
            " |  \n",
            " |  Iterating a :class:`StructType` will iterate over its :class:`StructField`\\s.\n",
            " |  A contained :class:`StructField` can be accessed by its name or position.\n",
            " |  \n",
            " |  Examples\n",
            " |  --------\n",
            " |  >>> from pyspark.sql.types import *\n",
            " |  >>> struct1 = StructType([StructField(\"f1\", StringType(), True)])\n",
            " |  >>> struct1[\"f1\"]\n",
            " |  StructField('f1', StringType(), True)\n",
            " |  >>> struct1[0]\n",
            " |  StructField('f1', StringType(), True)\n",
            " |  \n",
            " |  >>> struct1 = StructType([StructField(\"f1\", StringType(), True)])\n",
            " |  >>> struct2 = StructType([StructField(\"f1\", StringType(), True)])\n",
            " |  >>> struct1 == struct2\n",
            " |  True\n",
            " |  >>> struct1 = StructType([StructField(\"f1\", CharType(10), True)])\n",
            " |  >>> struct2 = StructType([StructField(\"f1\", CharType(10), True)])\n",
            " |  >>> struct1 == struct2\n",
            " |  True\n",
            " |  >>> struct1 = StructType([StructField(\"f1\", VarcharType(10), True)])\n",
            " |  >>> struct2 = StructType([StructField(\"f1\", VarcharType(10), True)])\n",
            " |  >>> struct1 == struct2\n",
            " |  True\n",
            " |  >>> struct1 = StructType([StructField(\"f1\", StringType(), True)])\n",
            " |  >>> struct2 = StructType([StructField(\"f1\", StringType(), True),\n",
            " |  ...     StructField(\"f2\", IntegerType(), False)])\n",
            " |  >>> struct1 == struct2\n",
            " |  False\n",
            " |  \n",
            " |  The below example demonstrates how to create a DataFrame based on a struct created\n",
            " |  using class:`StructType` and class:`StructField`:\n",
            " |  \n",
            " |  >>> data = [(\"Alice\", [\"Java\", \"Scala\"]), (\"Bob\", [\"Python\", \"Scala\"])]\n",
            " |  >>> schema = StructType([\n",
            " |  ...     StructField(\"name\", StringType()),\n",
            " |  ...     StructField(\"languagesSkills\", ArrayType(StringType())),\n",
            " |  ... ])\n",
            " |  >>> df = spark.createDataFrame(data=data, schema=schema)\n",
            " |  >>> df.printSchema()\n",
            " |  root\n",
            " |   |-- name: string (nullable = true)\n",
            " |   |-- languagesSkills: array (nullable = true)\n",
            " |   |    |-- element: string (containsNull = true)\n",
            " |  >>> df.show()\n",
            " |  +-----+---------------+\n",
            " |  | name|languagesSkills|\n",
            " |  +-----+---------------+\n",
            " |  |Alice|  [Java, Scala]|\n",
            " |  |  Bob|[Python, Scala]|\n",
            " |  +-----+---------------+\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      StructType\n",
            " |      DataType\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __getitem__(self, key: Union[str, int]) -> pyspark.sql.types.StructField\n",
            " |      Access fields by name or slice.\n",
            " |  \n",
            " |  __init__(self, fields: Optional[List[pyspark.sql.types.StructField]] = None)\n",
            " |      Initialize self.  See help(type(self)) for accurate signature.\n",
            " |  \n",
            " |  __iter__(self) -> Iterator[pyspark.sql.types.StructField]\n",
            " |      Iterate the fields\n",
            " |  \n",
            " |  __len__(self) -> int\n",
            " |      Return the number of fields.\n",
            " |  \n",
            " |  __repr__(self) -> str\n",
            " |      Return repr(self).\n",
            " |  \n",
            " |  add(self, field: Union[str, pyspark.sql.types.StructField], data_type: Union[str, pyspark.sql.types.DataType, NoneType] = None, nullable: bool = True, metadata: Optional[Dict[str, Any]] = None) -> 'StructType'\n",
            " |      Construct a :class:`StructType` by adding new elements to it, to define the schema.\n",
            " |      The method accepts either:\n",
            " |      \n",
            " |          a) A single parameter which is a :class:`StructField` object.\n",
            " |          b) Between 2 and 4 parameters as (name, data_type, nullable (optional),\n",
            " |             metadata(optional). The data_type parameter may be either a String or a\n",
            " |             :class:`DataType` object.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      field : str or :class:`StructField`\n",
            " |          Either the name of the field or a :class:`StructField` object\n",
            " |      data_type : :class:`DataType`, optional\n",
            " |          If present, the DataType of the :class:`StructField` to create\n",
            " |      nullable : bool, optional\n",
            " |          Whether the field to add should be nullable (default True)\n",
            " |      metadata : dict, optional\n",
            " |          Any additional metadata (default None)\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`StructType`\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> from pyspark.sql.types import IntegerType, StringType, StructField, StructType\n",
            " |      >>> struct1 = StructType().add(\"f1\", StringType(), True).add(\"f2\", StringType(), True, None)\n",
            " |      >>> struct2 = StructType([StructField(\"f1\", StringType(), True),\n",
            " |      ...     StructField(\"f2\", StringType(), True, None)])\n",
            " |      >>> struct1 == struct2\n",
            " |      True\n",
            " |      >>> struct1 = StructType().add(StructField(\"f1\", StringType(), True))\n",
            " |      >>> struct2 = StructType([StructField(\"f1\", StringType(), True)])\n",
            " |      >>> struct1 == struct2\n",
            " |      True\n",
            " |      >>> struct1 = StructType().add(\"f1\", \"string\", True)\n",
            " |      >>> struct2 = StructType([StructField(\"f1\", StringType(), True)])\n",
            " |      >>> struct1 == struct2\n",
            " |      True\n",
            " |  \n",
            " |  fieldNames(self) -> List[str]\n",
            " |      Returns all field names in a list.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> from pyspark.sql.types import StringType, StructField, StructType\n",
            " |      >>> struct = StructType([StructField(\"f1\", StringType(), True)])\n",
            " |      >>> struct.fieldNames()\n",
            " |      ['f1']\n",
            " |  \n",
            " |  fromInternal(self, obj: Tuple) -> 'Row'\n",
            " |      Converts an internal SQL object into a native Python object.\n",
            " |  \n",
            " |  jsonValue(self) -> Dict[str, Any]\n",
            " |  \n",
            " |  needConversion(self) -> bool\n",
            " |      Does this type needs conversion between Python object and internal SQL object.\n",
            " |      \n",
            " |      This is used to avoid the unnecessary conversion for ArrayType/MapType/StructType.\n",
            " |  \n",
            " |  simpleString(self) -> str\n",
            " |  \n",
            " |  toInternal(self, obj: Tuple) -> Tuple\n",
            " |      Converts a Python object into an internal SQL object.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods defined here:\n",
            " |  \n",
            " |  fromJson(json: Dict[str, Any]) -> 'StructType' from builtins.type\n",
            " |      Constructs :class:`StructType` from a schema defined in JSON format.\n",
            " |      \n",
            " |      Below is a JSON schema it must adhere to::\n",
            " |      \n",
            " |          {\n",
            " |            \"title\":\"StructType\",\n",
            " |            \"description\":\"Schema of StructType in json format\",\n",
            " |            \"type\":\"object\",\n",
            " |            \"properties\":{\n",
            " |               \"fields\":{\n",
            " |                  \"description\":\"Array of struct fields\",\n",
            " |                  \"type\":\"array\",\n",
            " |                  \"items\":{\n",
            " |                      \"type\":\"object\",\n",
            " |                      \"properties\":{\n",
            " |                         \"name\":{\n",
            " |                            \"description\":\"Name of the field\",\n",
            " |                            \"type\":\"string\"\n",
            " |                         },\n",
            " |                         \"type\":{\n",
            " |                            \"description\": \"Type of the field. Can either be\n",
            " |                                            another nested StructType or primitive type\",\n",
            " |                            \"type\":\"object/string\"\n",
            " |                         },\n",
            " |                         \"nullable\":{\n",
            " |                            \"description\":\"If nulls are allowed\",\n",
            " |                            \"type\":\"boolean\"\n",
            " |                         },\n",
            " |                         \"metadata\":{\n",
            " |                            \"description\":\"Additional metadata to supply\",\n",
            " |                            \"type\":\"object\"\n",
            " |                         },\n",
            " |                         \"required\":[\n",
            " |                            \"name\",\n",
            " |                            \"type\",\n",
            " |                            \"nullable\",\n",
            " |                            \"metadata\"\n",
            " |                         ]\n",
            " |                      }\n",
            " |                 }\n",
            " |              }\n",
            " |           }\n",
            " |         }\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      json : dict or a dict-like object e.g. JSON object\n",
            " |          This \"dict\" must have \"fields\" key that returns an array of fields\n",
            " |          each of which must have specific keys (name, type, nullable, metadata).\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`StructType`\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> json_str = '''\n",
            " |      ...  {\n",
            " |      ...      \"fields\": [\n",
            " |      ...          {\n",
            " |      ...              \"metadata\": {},\n",
            " |      ...              \"name\": \"Person\",\n",
            " |      ...              \"nullable\": true,\n",
            " |      ...              \"type\": {\n",
            " |      ...                  \"fields\": [\n",
            " |      ...                      {\n",
            " |      ...                          \"metadata\": {},\n",
            " |      ...                          \"name\": \"name\",\n",
            " |      ...                          \"nullable\": false,\n",
            " |      ...                          \"type\": \"string\"\n",
            " |      ...                      },\n",
            " |      ...                      {\n",
            " |      ...                          \"metadata\": {},\n",
            " |      ...                          \"name\": \"surname\",\n",
            " |      ...                          \"nullable\": false,\n",
            " |      ...                          \"type\": \"string\"\n",
            " |      ...                      }\n",
            " |      ...                  ],\n",
            " |      ...                  \"type\": \"struct\"\n",
            " |      ...              }\n",
            " |      ...          }\n",
            " |      ...      ],\n",
            " |      ...      \"type\": \"struct\"\n",
            " |      ...  }\n",
            " |      ...  '''\n",
            " |      >>> import json\n",
            " |      >>> scheme = StructType.fromJson(json.loads(json_str))\n",
            " |      >>> scheme.simpleString()\n",
            " |      'struct<Person:struct<name:string,surname:string>>'\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from DataType:\n",
            " |  \n",
            " |  __eq__(self, other: Any) -> bool\n",
            " |      Return self==value.\n",
            " |  \n",
            " |  __hash__(self) -> int\n",
            " |      Return hash(self).\n",
            " |  \n",
            " |  __ne__(self, other: Any) -> bool\n",
            " |      Return self!=value.\n",
            " |  \n",
            " |  json(self) -> str\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods inherited from DataType:\n",
            " |  \n",
            " |  typeName() -> str from builtins.type\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from DataType:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.types import *\n",
        "help(StructType)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_KjncgpPmw0l",
        "outputId": "ce282465-00c9-4089-bc9d-01e4b3690384"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+---------+\n",
            "| id|     name|\n",
            "+---+---------+\n",
            "|  1|    Pedro|\n",
            "|  2|Guilherme|\n",
            "+---+---------+\n",
            "\n",
            "root\n",
            " |-- id: integer (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "data = [(1,'Pedro'), (2, 'Guilherme')]\n",
        "schema = StructType([StructField(name='id', dataType=IntegerType()),\n",
        "                     StructField(name='name', dataType=StringType())])\n",
        "\n",
        "df = spark.createDataFrame(data=data, schema=schema)\n",
        "\n",
        "df.show()\n",
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LV7kQI0MsZ1o",
        "outputId": "759a0e63-1227-4454-ef12-407712d5ac79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+---------+\n",
            "| id|     name|\n",
            "+---+---------+\n",
            "|  1|    Pedro|\n",
            "|  2|Guilherme|\n",
            "+---+---------+\n",
            "\n",
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "data = [{'id':1, 'name':'Pedro'},\n",
        "        {'id':2, 'name':'Guilherme'}]\n",
        "\n",
        "df = spark.createDataFrame(data=data)\n",
        "\n",
        "df.show()\n",
        "df.printSchema()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyP+1RP879nJBEnoQGT8HGVO",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
