{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmlEeVvys-2Y"
      },
      "source": [
        "Based on [WafaStudies](https://www.youtube.com/@WafaStudies) PySpark [tutorial](https://www.youtube.com/playlist?list=PLMWaZteqtEaJFiJ2FyIKK0YEuXwQ9YIS_)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Writing dataframe to CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| default_exp Writing dataframe to CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| hide\n",
        "from nbdev.showdoc import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnQUnkcLjqcA"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "hbpmyFRaEeyf"
      },
      "outputs": [],
      "source": [
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "J-69JCOeEgVZ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "your 131072x1 screen size is bogus. expect trouble\n",
            "23/10/25 15:11:26 WARN Utils: Your hostname, PC resolves to a loopback address: 127.0.1.1; using 172.29.148.244 instead (on interface eth0)\n",
            "23/10/25 15:11:26 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
            "23/10/25 15:11:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "23/10/25 15:11:28 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
            "23/10/25 15:11:28 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "spark = SparkSession.builder\\\n",
        "                    .appName('Spark')\\\n",
        "                    .master(\"local[*]\")\\\n",
        "                    .getOrCreate()\n",
        "\n",
        "from pyspark.sql import dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rUyt7_kuo3b"
      },
      "source": [
        "## Writing dataframe to CSV files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M8veWpzo54Q3",
        "outputId": "9a1a4275-8bab-4417-98a1-0734c51a904c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Help on property:\n",
            "\n",
            "    Interface for saving the content of the non-streaming :class:`DataFrame` out into external\n",
            "    storage.\n",
            "    \n",
            "    .. versionadded:: 1.4.0\n",
            "    \n",
            "    .. versionchanged:: 3.4.0\n",
            "        Supports Spark Connect.\n",
            "    \n",
            "    Returns\n",
            "    -------\n",
            "    :class:`DataFrameWriter`\n",
            "    \n",
            "    Examples\n",
            "    --------\n",
            "    >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
            "    >>> type(df.write)\n",
            "    <class '...readwriter.DataFrameWriter'>\n",
            "    \n",
            "    Write the DataFrame as a table.\n",
            "    \n",
            "    >>> _ = spark.sql(\"DROP TABLE IF EXISTS tab2\")\n",
            "    >>> df.write.saveAsTable(\"tab2\")\n",
            "    >>> _ = spark.sql(\"DROP TABLE tab2\")\n",
            "\n"
          ]
        }
      ],
      "source": [
        "help(dataframe.DataFrame.write)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azh69oRC9HNi"
      },
      "source": [
        "Let's start creating a dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9YC5V3fq6Ozb",
        "outputId": "00e09c92-a88e-4342-d470-d4c4965a2894"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+-----+\n",
            "|age| name|\n",
            "+---+-----+\n",
            "|  2|Alice|\n",
            "|  5|  Bob|\n",
            "+---+-----+\n",
            "\n",
            "root\n",
            " |-- age: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
        "\n",
        "df.show()\n",
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTEZvEKX9hzC",
        "outputId": "a54afa7e-904c-4282-caa4-e5ec68d718ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Help on method csv in module pyspark.sql.readwriter:\n",
            "\n",
            "csv(path: str, mode: Optional[str] = None, compression: Optional[str] = None, sep: Optional[str] = None, quote: Optional[str] = None, escape: Optional[str] = None, header: Union[bool, str, NoneType] = None, nullValue: Optional[str] = None, escapeQuotes: Union[bool, str, NoneType] = None, quoteAll: Union[bool, str, NoneType] = None, dateFormat: Optional[str] = None, timestampFormat: Optional[str] = None, ignoreLeadingWhiteSpace: Union[bool, str, NoneType] = None, ignoreTrailingWhiteSpace: Union[bool, str, NoneType] = None, charToEscapeQuoteEscaping: Optional[str] = None, encoding: Optional[str] = None, emptyValue: Optional[str] = None, lineSep: Optional[str] = None) -> None method of pyspark.sql.readwriter.DataFrameWriter instance\n",
            "    Saves the content of the :class:`DataFrame` in CSV format at the specified path.\n",
            "    \n",
            "    .. versionadded:: 2.0.0\n",
            "    \n",
            "    .. versionchanged:: 3.4.0\n",
            "        Supports Spark Connect.\n",
            "    \n",
            "    Parameters\n",
            "    ----------\n",
            "    path : str\n",
            "        the path in any Hadoop supported file system\n",
            "    mode : str, optional\n",
            "        specifies the behavior of the save operation when data already exists.\n",
            "    \n",
            "        * ``append``: Append contents of this :class:`DataFrame` to existing data.\n",
            "        * ``overwrite``: Overwrite existing data.\n",
            "        * ``ignore``: Silently ignore this operation if data already exists.\n",
            "        * ``error`` or ``errorifexists`` (default case): Throw an exception if data already \\\n",
            "            exists.\n",
            "    \n",
            "    Other Parameters\n",
            "    ----------------\n",
            "    Extra options\n",
            "        For the extra options, refer to\n",
            "        `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-csv.html#data-source-option>`_\n",
            "        for the version you use.\n",
            "    \n",
            "        .. # noqa\n",
            "    \n",
            "    Examples\n",
            "    --------\n",
            "    Write a DataFrame into a CSV file and read it back.\n",
            "    \n",
            "    >>> import tempfile\n",
            "    >>> with tempfile.TemporaryDirectory() as d:\n",
            "    ...     # Write a DataFrame into a CSV file\n",
            "    ...     df = spark.createDataFrame([{\"age\": 100, \"name\": \"Hyukjin Kwon\"}])\n",
            "    ...     df.write.csv(d, mode=\"overwrite\")\n",
            "    ...\n",
            "    ...     # Read the CSV file as a DataFrame with 'nullValue' option set to 'Hyukjin Kwon'.\n",
            "    ...     spark.read.schema(df.schema).format(\"csv\").option(\n",
            "    ...         \"nullValue\", \"Hyukjin Kwon\").load(d).show()\n",
            "    +---+----+\n",
            "    |age|name|\n",
            "    +---+----+\n",
            "    |100|NULL|\n",
            "    +---+----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "help(df.write.csv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ntDjIxP89MyN"
      },
      "outputs": [],
      "source": [
        "df.write\\\n",
        ".csv(\"df_csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rGnNlFE950e"
      },
      "source": [
        "Let's check the written file:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HI8u7Ya198B5",
        "outputId": "c15ddc5e-5a7d-42b2-bc58-2ef2c19b0153"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+-----+\n",
            "|age| name|\n",
            "+---+-----+\n",
            "|  2|Alice|\n",
            "|  5|  Bob|\n",
            "+---+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.read.schema(df.schema)\\\n",
        ".format(\"csv\")\\\n",
        ".load(\"df_csv\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASkN0u1uCBEu"
      },
      "source": [
        "What if we want to change the dataframe?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGW6JC7VCKQ-",
        "outputId": "32fc53c6-b5ac-4a6f-b5b8-55af6acf5001"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+------+\n",
            "| id|  name|\n",
            "+---+------+\n",
            "|  1|  Goku|\n",
            "|  2|Naruto|\n",
            "+---+------+\n",
            "\n",
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df = spark.createDataFrame([(1, \"Goku\"), (2, \"Naruto\")], schema=[\"id\", \"name\"])\n",
        "\n",
        "df.show()\n",
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "N2QK_NQzCSRw",
        "outputId": "735d6cac-922c-44a5-bd19-838c05ef17e1"
      },
      "outputs": [
        {
          "ename": "AnalysisException",
          "evalue": "[PATH_ALREADY_EXISTS] Path file:/home/pedro-wsl/Learning-PySpark/nbs/df_csv already exists. Set mode as \"overwrite\" to overwrite the existing path.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/home/pedro-wsl/Learning-PySpark/nbs/02_Writing-dataframe-to-csv.ipynb Cell 18\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pedro-wsl/Learning-PySpark/nbs/02_Writing-dataframe-to-csv.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m df\u001b[39m.\u001b[39;49mwrite\\\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pedro-wsl/Learning-PySpark/nbs/02_Writing-dataframe-to-csv.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m.\u001b[39;49mcsv(\u001b[39m\"\u001b[39;49m\u001b[39mdf_csv\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
            "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py:1864\u001b[0m, in \u001b[0;36mDataFrameWriter.csv\u001b[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[1;32m   1845\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode(mode)\n\u001b[1;32m   1846\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_opts(\n\u001b[1;32m   1847\u001b[0m     compression\u001b[39m=\u001b[39mcompression,\n\u001b[1;32m   1848\u001b[0m     sep\u001b[39m=\u001b[39msep,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1862\u001b[0m     lineSep\u001b[39m=\u001b[39mlineSep,\n\u001b[1;32m   1863\u001b[0m )\n\u001b[0;32m-> 1864\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jwrite\u001b[39m.\u001b[39;49mcsv(path)\n",
            "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
            "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [PATH_ALREADY_EXISTS] Path file:/home/pedro-wsl/Learning-PySpark/nbs/df_csv already exists. Set mode as \"overwrite\" to overwrite the existing path."
          ]
        }
      ],
      "source": [
        "df.write\\\n",
        ".csv(\"df_csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFSXl734CUIF"
      },
      "source": [
        "It will give us an error because the file already exists, so we need to overwrite it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "VZ8fP5Ti8k2H"
      },
      "outputs": [],
      "source": [
        "df.write\\\n",
        ".csv(\"df_csv\", mode=\"overwrite\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tRKwWb0CZLS"
      },
      "source": [
        "Let's check:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMlJmMFq8u8V",
        "outputId": "7ac0d820-8cbc-4183-d1f3-6a098f3d97ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+------+\n",
            "| id|  name|\n",
            "+---+------+\n",
            "|  2|Naruto|\n",
            "|  1|  Goku|\n",
            "+---+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.read.schema(df.schema)\\\n",
        ".format(\"csv\")\\\n",
        ".load(\"df_csv\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbwT_leuChKg"
      },
      "source": [
        "And how to add more items to the file?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiIMEkjqDH9M"
      },
      "source": [
        "Let's create another dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSA2_sc7CkeF",
        "outputId": "b039492e-9c76-4303-df87-fe9757e13448"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+------+\n",
            "| id|  name|\n",
            "+---+------+\n",
            "|  3|  Gojo|\n",
            "|  4|Kirito|\n",
            "+---+------+\n",
            "\n",
            "root\n",
            " |-- id: string (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df2 = spark.createDataFrame([(\"3\", \"Gojo\"), (\"4\", \"Kirito\")], schema=[\"id\", \"name\"])\n",
        "\n",
        "df2.show()\n",
        "df2.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWEa52kMDKJo"
      },
      "source": [
        "Then we have to append it to the file:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "GoCYLq49C82N"
      },
      "outputs": [],
      "source": [
        "df2.write\\\n",
        ".csv(\"df_csv\", mode=\"append\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ss6eKX9MDCSa",
        "outputId": "60616487-e025-407b-d3fe-8514ff8ad0bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+------+\n",
            "| id|  name|\n",
            "+---+------+\n",
            "|  2|Naruto|\n",
            "|  4|Kirito|\n",
            "|  3|  Gojo|\n",
            "|  1|  Goku|\n",
            "+---+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.read.schema(df.schema)\\\n",
        ".format(\"csv\")\\\n",
        ".load(\"df_csv\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLInTON7EL-V"
      },
      "source": [
        "### File structure:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xm1v7BvgEO6g"
      },
      "source": [
        "Let's check the csv file structure:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t6GjzfZiELiX",
        "outputId": "0941e7e4-14fd-4c2e-e24e-fc332c2e172f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "directory\n"
          ]
        }
      ],
      "source": [
        "!file -b df_csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QoPpW3YREjqy"
      },
      "source": [
        "It's a folder, let's check it's content:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DPk64KE3ETAl",
        "outputId": "427384c0-c521-4b4f-aa39-a1965d5e4ff5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 52\n",
            "drwxr-xr-x 2 pedro-wsl pedro-wsl 4096 Oct 25 15:13 .\n",
            "drwxr-xr-x 5 pedro-wsl pedro-wsl 4096 Oct 25 15:13 ..\n",
            "-rw-r--r-- 1 pedro-wsl pedro-wsl    8 Oct 25 15:13 ._SUCCESS.crc\n",
            "-rw-r--r-- 1 pedro-wsl pedro-wsl    8 Oct 25 15:13 .part-00000-1d555ec3-1910-480c-b90f-23b1a97a974c-c000.csv.crc\n",
            "-rw-r--r-- 1 pedro-wsl pedro-wsl    8 Oct 25 15:13 .part-00000-ab8126fc-3fdb-44d1-8d0a-c7556dbe19c0-c000.csv.crc\n",
            "-rw-r--r-- 1 pedro-wsl pedro-wsl   12 Oct 25 15:13 .part-00005-1d555ec3-1910-480c-b90f-23b1a97a974c-c000.csv.crc\n",
            "-rw-r--r-- 1 pedro-wsl pedro-wsl   12 Oct 25 15:13 .part-00005-ab8126fc-3fdb-44d1-8d0a-c7556dbe19c0-c000.csv.crc\n",
            "-rw-r--r-- 1 pedro-wsl pedro-wsl   12 Oct 25 15:13 .part-00011-1d555ec3-1910-480c-b90f-23b1a97a974c-c000.csv.crc\n",
            "-rw-r--r-- 1 pedro-wsl pedro-wsl   12 Oct 25 15:13 .part-00011-ab8126fc-3fdb-44d1-8d0a-c7556dbe19c0-c000.csv.crc\n",
            "-rw-r--r-- 1 pedro-wsl pedro-wsl    0 Oct 25 15:13 _SUCCESS\n",
            "-rw-r--r-- 1 pedro-wsl pedro-wsl    0 Oct 25 15:13 part-00000-1d555ec3-1910-480c-b90f-23b1a97a974c-c000.csv\n",
            "-rw-r--r-- 1 pedro-wsl pedro-wsl    0 Oct 25 15:13 part-00000-ab8126fc-3fdb-44d1-8d0a-c7556dbe19c0-c000.csv\n",
            "-rw-r--r-- 1 pedro-wsl pedro-wsl    7 Oct 25 15:13 part-00005-1d555ec3-1910-480c-b90f-23b1a97a974c-c000.csv\n",
            "-rw-r--r-- 1 pedro-wsl pedro-wsl    7 Oct 25 15:13 part-00005-ab8126fc-3fdb-44d1-8d0a-c7556dbe19c0-c000.csv\n",
            "-rw-r--r-- 1 pedro-wsl pedro-wsl    9 Oct 25 15:13 part-00011-1d555ec3-1910-480c-b90f-23b1a97a974c-c000.csv\n",
            "-rw-r--r-- 1 pedro-wsl pedro-wsl    9 Oct 25 15:13 part-00011-ab8126fc-3fdb-44d1-8d0a-c7556dbe19c0-c000.csv\n"
          ]
        }
      ],
      "source": [
        "!ls -la df_csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LM75G8v8EwRp"
      },
      "source": [
        "It's divided in partitions\n",
        "\n",
        "The number of partitions is the same number of rows we have on ```df```\n",
        "\n",
        "This happens because spark have a ```driver node``` that divide the workload between ```worker nodes```, like:\n",
        "\n",
        "      Driver Node\n",
        "     /   |    |   \\\n",
        "    W1   W2   W3   W4\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cWXMvfmG7rQ"
      },
      "source": [
        "We can also specify the number of partitions we want:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ttTSTJSEoQm",
        "outputId": "65e29719-3e64-4905-e44a-84a14ebbbf23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Help on method repartition in module pyspark.sql.dataframe:\n",
            "\n",
            "repartition(numPartitions: Union[int, ForwardRef('ColumnOrName')], *cols: 'ColumnOrName') -> 'DataFrame' method of pyspark.sql.dataframe.DataFrame instance\n",
            "    Returns a new :class:`DataFrame` partitioned by the given partitioning expressions. The\n",
            "    resulting :class:`DataFrame` is hash partitioned.\n",
            "    \n",
            "    .. versionadded:: 1.3.0\n",
            "    \n",
            "    .. versionchanged:: 3.4.0\n",
            "        Supports Spark Connect.\n",
            "    \n",
            "    Parameters\n",
            "    ----------\n",
            "    numPartitions : int\n",
            "        can be an int to specify the target number of partitions or a Column.\n",
            "        If it is a Column, it will be used as the first partitioning column. If not specified,\n",
            "        the default number of partitions is used.\n",
            "    cols : str or :class:`Column`\n",
            "        partitioning columns.\n",
            "    \n",
            "        .. versionchanged:: 1.6.0\n",
            "           Added optional arguments to specify the partitioning columns. Also made numPartitions\n",
            "           optional if partitioning columns are specified.\n",
            "    \n",
            "    Returns\n",
            "    -------\n",
            "    :class:`DataFrame`\n",
            "        Repartitioned DataFrame.\n",
            "    \n",
            "    Examples\n",
            "    --------\n",
            "    >>> df = spark.createDataFrame(\n",
            "    ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
            "    \n",
            "    Repartition the data into 10 partitions.\n",
            "    \n",
            "    >>> df.repartition(10).rdd.getNumPartitions()\n",
            "    10\n",
            "    \n",
            "    Repartition the data into 7 partitions by 'age' column.\n",
            "    \n",
            "    >>> df.repartition(7, \"age\").rdd.getNumPartitions()\n",
            "    7\n",
            "    \n",
            "    Repartition the data into 7 partitions by 'age' and 'name columns.\n",
            "    \n",
            "    >>> df.repartition(3, \"name\", \"age\").rdd.getNumPartitions()\n",
            "    3\n",
            "\n"
          ]
        }
      ],
      "source": [
        "help(df.repartition)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sna0-VLUHBwa",
        "outputId": "f8d868f8-2e17-4fed-9b01-f942c573c0c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+------+\n",
            "| id|  name|\n",
            "+---+------+\n",
            "|  1|  Goku|\n",
            "|  2|Naruto|\n",
            "+---+------+\n",
            "\n",
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df = spark.createDataFrame([(1, \"Goku\"), (2, \"Naruto\")], schema=[\"id\", \"name\"])\n",
        "\n",
        "df.show()\n",
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxBZC8laHBwa",
        "outputId": "7282c62c-cb0b-4680-e2f7-44d428b46559"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_1part = df.repartition(1)\n",
        "df_1part.rdd.getNumPartitions()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "eQHjXI73IDcB"
      },
      "outputs": [],
      "source": [
        "df_1part\\\n",
        ".write\\\n",
        ".csv(\"df_csv_1part\", mode=\"overwrite\", header=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZxP6Gn4cHIEx",
        "outputId": "c70b6353-cb8a-4b7b-b3f9-1f393414c2c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+------+\n",
            "| id|  name|\n",
            "+---+------+\n",
            "|  1|  Goku|\n",
            "|  2|Naruto|\n",
            "+---+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.read\\\n",
        ".option(\"header\", True)\\\n",
        ".format(\"csv\")\\\n",
        ".load(\"df_csv_1part\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "StNa-58uHWuh",
        "outputId": "3970ddb9-59e9-4b14-cebd-56f45e5a58e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 20\n",
            "drwxr-xr-x 2 pedro-wsl pedro-wsl 4096 Oct 25 15:13 .\n",
            "drwxr-xr-x 6 pedro-wsl pedro-wsl 4096 Oct 25 15:13 ..\n",
            "-rw-r--r-- 1 pedro-wsl pedro-wsl    8 Oct 25 15:13 ._SUCCESS.crc\n",
            "-rw-r--r-- 1 pedro-wsl pedro-wsl   12 Oct 25 15:13 .part-00000-48e65ab0-a0d2-4a48-8f0c-e0b85439f207-c000.csv.crc\n",
            "-rw-r--r-- 1 pedro-wsl pedro-wsl    0 Oct 25 15:13 _SUCCESS\n",
            "-rw-r--r-- 1 pedro-wsl pedro-wsl   24 Oct 25 15:13 part-00000-48e65ab0-a0d2-4a48-8f0c-e0b85439f207-c000.csv\n"
          ]
        }
      ],
      "source": [
        "!ls -la df_csv_1part/"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyM3uQ37Y6w9fBeD8ej1ySHQ",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
