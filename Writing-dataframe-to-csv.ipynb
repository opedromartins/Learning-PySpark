{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM3uQ37Y6w9fBeD8ej1ySHQ"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Based on [WafaStudies](https://www.youtube.com/@WafaStudies) PySpark [tutorial](https://www.youtube.com/playlist?list=PLMWaZteqtEaJFiJ2FyIKK0YEuXwQ9YIS_)."
      ],
      "metadata": {
        "id": "CmlEeVvys-2Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "DnQUnkcLjqcA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz\n",
        "!tar xf spark-3.5.0-bin-hadoop3.tgz\n",
        "!pip -q install findspark"
      ],
      "metadata": {
        "id": "AMJqMeKMEby2"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.0-bin-hadoop3\""
      ],
      "metadata": {
        "id": "YCAEgkYiEdV2"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "hbpmyFRaEeyf"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "spark = SparkSession.builder\\\n",
        "                    .appName('Spark')\\\n",
        "                    .master(\"local[*]\")\\\n",
        "                    .getOrCreate()\n",
        "\n",
        "from pyspark.sql import dataframe"
      ],
      "metadata": {
        "id": "J-69JCOeEgVZ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Writing dataframe to CSV files"
      ],
      "metadata": {
        "id": "7rUyt7_kuo3b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "help(dataframe.DataFrame.write)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M8veWpzo54Q3",
        "outputId": "9a1a4275-8bab-4417-98a1-0734c51a904c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on property:\n",
            "\n",
            "    Interface for saving the content of the non-streaming :class:`DataFrame` out into external\n",
            "    storage.\n",
            "    \n",
            "    .. versionadded:: 1.4.0\n",
            "    \n",
            "    .. versionchanged:: 3.4.0\n",
            "        Supports Spark Connect.\n",
            "    \n",
            "    Returns\n",
            "    -------\n",
            "    :class:`DataFrameWriter`\n",
            "    \n",
            "    Examples\n",
            "    --------\n",
            "    >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
            "    >>> type(df.write)\n",
            "    <class '...readwriter.DataFrameWriter'>\n",
            "    \n",
            "    Write the DataFrame as a table.\n",
            "    \n",
            "    >>> _ = spark.sql(\"DROP TABLE IF EXISTS tab2\")\n",
            "    >>> df.write.saveAsTable(\"tab2\")\n",
            "    >>> _ = spark.sql(\"DROP TABLE tab2\")\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start creating a dataframe"
      ],
      "metadata": {
        "id": "azh69oRC9HNi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
        "\n",
        "df.show()\n",
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9YC5V3fq6Ozb",
        "outputId": "00e09c92-a88e-4342-d470-d4c4965a2894"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+\n",
            "|age| name|\n",
            "+---+-----+\n",
            "|  2|Alice|\n",
            "|  5|  Bob|\n",
            "+---+-----+\n",
            "\n",
            "root\n",
            " |-- age: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "help(df.write.csv)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTEZvEKX9hzC",
        "outputId": "a54afa7e-904c-4282-caa4-e5ec68d718ca"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on method csv in module pyspark.sql.readwriter:\n",
            "\n",
            "csv(path: str, mode: Optional[str] = None, compression: Optional[str] = None, sep: Optional[str] = None, quote: Optional[str] = None, escape: Optional[str] = None, header: Union[bool, str, NoneType] = None, nullValue: Optional[str] = None, escapeQuotes: Union[bool, str, NoneType] = None, quoteAll: Union[bool, str, NoneType] = None, dateFormat: Optional[str] = None, timestampFormat: Optional[str] = None, ignoreLeadingWhiteSpace: Union[bool, str, NoneType] = None, ignoreTrailingWhiteSpace: Union[bool, str, NoneType] = None, charToEscapeQuoteEscaping: Optional[str] = None, encoding: Optional[str] = None, emptyValue: Optional[str] = None, lineSep: Optional[str] = None) -> None method of pyspark.sql.readwriter.DataFrameWriter instance\n",
            "    Saves the content of the :class:`DataFrame` in CSV format at the specified path.\n",
            "    \n",
            "    .. versionadded:: 2.0.0\n",
            "    \n",
            "    .. versionchanged:: 3.4.0\n",
            "        Supports Spark Connect.\n",
            "    \n",
            "    Parameters\n",
            "    ----------\n",
            "    path : str\n",
            "        the path in any Hadoop supported file system\n",
            "    mode : str, optional\n",
            "        specifies the behavior of the save operation when data already exists.\n",
            "    \n",
            "        * ``append``: Append contents of this :class:`DataFrame` to existing data.\n",
            "        * ``overwrite``: Overwrite existing data.\n",
            "        * ``ignore``: Silently ignore this operation if data already exists.\n",
            "        * ``error`` or ``errorifexists`` (default case): Throw an exception if data already \\\n",
            "            exists.\n",
            "    \n",
            "    Other Parameters\n",
            "    ----------------\n",
            "    Extra options\n",
            "        For the extra options, refer to\n",
            "        `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-csv.html#data-source-option>`_\n",
            "        for the version you use.\n",
            "    \n",
            "        .. # noqa\n",
            "    \n",
            "    Examples\n",
            "    --------\n",
            "    Write a DataFrame into a CSV file and read it back.\n",
            "    \n",
            "    >>> import tempfile\n",
            "    >>> with tempfile.TemporaryDirectory() as d:\n",
            "    ...     # Write a DataFrame into a CSV file\n",
            "    ...     df = spark.createDataFrame([{\"age\": 100, \"name\": \"Hyukjin Kwon\"}])\n",
            "    ...     df.write.csv(d, mode=\"overwrite\")\n",
            "    ...\n",
            "    ...     # Read the CSV file as a DataFrame with 'nullValue' option set to 'Hyukjin Kwon'.\n",
            "    ...     spark.read.schema(df.schema).format(\"csv\").option(\n",
            "    ...         \"nullValue\", \"Hyukjin Kwon\").load(d).show()\n",
            "    +---+----+\n",
            "    |age|name|\n",
            "    +---+----+\n",
            "    |100|NULL|\n",
            "    +---+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.write\\\n",
        ".csv(\"df_csv\")"
      ],
      "metadata": {
        "id": "ntDjIxP89MyN"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check the written file:"
      ],
      "metadata": {
        "id": "-rGnNlFE950e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.read.schema(df.schema)\\\n",
        ".format(\"csv\")\\\n",
        ".load(\"df_csv\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HI8u7Ya198B5",
        "outputId": "c15ddc5e-5a7d-42b2-bc58-2ef2c19b0153"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+\n",
            "|age| name|\n",
            "+---+-----+\n",
            "|  2|Alice|\n",
            "|  5|  Bob|\n",
            "+---+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What if we want to change the dataframe?"
      ],
      "metadata": {
        "id": "ASkN0u1uCBEu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.createDataFrame([(1, \"Goku\"), (2, \"Naruto\")], schema=[\"id\", \"name\"])\n",
        "\n",
        "df.show()\n",
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGW6JC7VCKQ-",
        "outputId": "32fc53c6-b5ac-4a6f-b5b8-55af6acf5001"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+\n",
            "| id|  name|\n",
            "+---+------+\n",
            "|  1|  Goku|\n",
            "|  2|Naruto|\n",
            "+---+------+\n",
            "\n",
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.write\\\n",
        ".csv(\"df_csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "N2QK_NQzCSRw",
        "outputId": "735d6cac-922c-44a5-bd19-838c05ef17e1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-b3e931cfcd03>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"df_csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/spark-3.5.0-bin-hadoop3/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[1;32m   1862\u001b[0m             \u001b[0mlineSep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlineSep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1863\u001b[0m         )\n\u001b[0;32m-> 1864\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1866\u001b[0m     def orc(\n",
            "\u001b[0;32m/content/spark-3.5.0-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.5.0-bin-hadoop3/python/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [PATH_ALREADY_EXISTS] Path file:/content/df_csv already exists. Set mode as \"overwrite\" to overwrite the existing path."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It will give us an error because the file already exists, so we need to overwrite it:"
      ],
      "metadata": {
        "id": "pFSXl734CUIF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.write\\\n",
        ".csv(\"df_csv\", mode=\"overwrite\")"
      ],
      "metadata": {
        "id": "VZ8fP5Ti8k2H"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check:"
      ],
      "metadata": {
        "id": "0tRKwWb0CZLS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.read.schema(df.schema)\\\n",
        ".format(\"csv\")\\\n",
        ".load(\"df_csv\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMlJmMFq8u8V",
        "outputId": "7ac0d820-8cbc-4183-d1f3-6a098f3d97ef"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+\n",
            "| id|  name|\n",
            "+---+------+\n",
            "|  2|Naruto|\n",
            "|  1|  Goku|\n",
            "+---+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And how to add more items to the file?"
      ],
      "metadata": {
        "id": "qbwT_leuChKg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create another dataframe:"
      ],
      "metadata": {
        "id": "OiIMEkjqDH9M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = spark.createDataFrame([(\"3\", \"Gojo\"), (\"4\", \"Kirito\")], schema=[\"id\", \"name\"])\n",
        "\n",
        "df2.show()\n",
        "df2.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSA2_sc7CkeF",
        "outputId": "b039492e-9c76-4303-df87-fe9757e13448"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+\n",
            "| id|  name|\n",
            "+---+------+\n",
            "|  3|  Gojo|\n",
            "|  4|Kirito|\n",
            "+---+------+\n",
            "\n",
            "root\n",
            " |-- id: string (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we have to append it to the file:"
      ],
      "metadata": {
        "id": "yWEa52kMDKJo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df2.write\\\n",
        ".csv(\"df_csv\", mode=\"append\")"
      ],
      "metadata": {
        "id": "GoCYLq49C82N"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.read.schema(df.schema)\\\n",
        ".format(\"csv\")\\\n",
        ".load(\"df_csv\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ss6eKX9MDCSa",
        "outputId": "60616487-e025-407b-d3fe-8514ff8ad0bb"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+\n",
            "| id|  name|\n",
            "+---+------+\n",
            "|  2|Naruto|\n",
            "|  4|Kirito|\n",
            "|  1|  Goku|\n",
            "|  3|  Gojo|\n",
            "+---+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### File structure:"
      ],
      "metadata": {
        "id": "KLInTON7EL-V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check the csv file structure:"
      ],
      "metadata": {
        "id": "Xm1v7BvgEO6g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!file -b df_csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t6GjzfZiELiX",
        "outputId": "0941e7e4-14fd-4c2e-e24e-fc332c2e172f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's a folder, let's check it's content:"
      ],
      "metadata": {
        "id": "QoPpW3YREjqy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -la df_csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DPk64KE3ETAl",
        "outputId": "427384c0-c521-4b4f-aa39-a1965d5e4ff5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 44\n",
            "drwxr-xr-x 2 root root 4096 Oct  9 11:20 .\n",
            "drwxr-xr-x 1 root root 4096 Oct  9 11:20 ..\n",
            "-rw-r--r-- 1 root root    7 Oct  9 11:20 part-00000-3c472559-2b7c-40b9-97ca-2c678d1510a1-c000.csv\n",
            "-rw-r--r-- 1 root root   12 Oct  9 11:20 .part-00000-3c472559-2b7c-40b9-97ca-2c678d1510a1-c000.csv.crc\n",
            "-rw-r--r-- 1 root root    7 Oct  9 11:20 part-00000-acb262e5-9c95-4ef3-8542-ac136f339622-c000.csv\n",
            "-rw-r--r-- 1 root root   12 Oct  9 11:20 .part-00000-acb262e5-9c95-4ef3-8542-ac136f339622-c000.csv.crc\n",
            "-rw-r--r-- 1 root root    9 Oct  9 11:20 part-00001-3c472559-2b7c-40b9-97ca-2c678d1510a1-c000.csv\n",
            "-rw-r--r-- 1 root root   12 Oct  9 11:20 .part-00001-3c472559-2b7c-40b9-97ca-2c678d1510a1-c000.csv.crc\n",
            "-rw-r--r-- 1 root root    9 Oct  9 11:20 part-00001-acb262e5-9c95-4ef3-8542-ac136f339622-c000.csv\n",
            "-rw-r--r-- 1 root root   12 Oct  9 11:20 .part-00001-acb262e5-9c95-4ef3-8542-ac136f339622-c000.csv.crc\n",
            "-rw-r--r-- 1 root root    0 Oct  9 11:20 _SUCCESS\n",
            "-rw-r--r-- 1 root root    8 Oct  9 11:20 ._SUCCESS.crc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's divided in partitions\n",
        "\n",
        "The number of partitions is the same number of rows we have on ```df```\n",
        "\n",
        "This happens because spark have a ```driver node``` that divide the workload between ```worker nodes```, like:\n",
        "\n",
        "      Driver Node\n",
        "     /   |    |   \\\n",
        "    W1   W2   W3   W4\n"
      ],
      "metadata": {
        "id": "LM75G8v8EwRp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also specify the number of partitions we want:"
      ],
      "metadata": {
        "id": "3cWXMvfmG7rQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "help(df.repartition)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ttTSTJSEoQm",
        "outputId": "65e29719-3e64-4905-e44a-84a14ebbbf23"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on method repartition in module pyspark.sql.dataframe:\n",
            "\n",
            "repartition(numPartitions: Union[int, ForwardRef('ColumnOrName')], *cols: 'ColumnOrName') -> 'DataFrame' method of pyspark.sql.dataframe.DataFrame instance\n",
            "    Returns a new :class:`DataFrame` partitioned by the given partitioning expressions. The\n",
            "    resulting :class:`DataFrame` is hash partitioned.\n",
            "    \n",
            "    .. versionadded:: 1.3.0\n",
            "    \n",
            "    .. versionchanged:: 3.4.0\n",
            "        Supports Spark Connect.\n",
            "    \n",
            "    Parameters\n",
            "    ----------\n",
            "    numPartitions : int\n",
            "        can be an int to specify the target number of partitions or a Column.\n",
            "        If it is a Column, it will be used as the first partitioning column. If not specified,\n",
            "        the default number of partitions is used.\n",
            "    cols : str or :class:`Column`\n",
            "        partitioning columns.\n",
            "    \n",
            "        .. versionchanged:: 1.6.0\n",
            "           Added optional arguments to specify the partitioning columns. Also made numPartitions\n",
            "           optional if partitioning columns are specified.\n",
            "    \n",
            "    Returns\n",
            "    -------\n",
            "    :class:`DataFrame`\n",
            "        Repartitioned DataFrame.\n",
            "    \n",
            "    Examples\n",
            "    --------\n",
            "    >>> df = spark.createDataFrame(\n",
            "    ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
            "    \n",
            "    Repartition the data into 10 partitions.\n",
            "    \n",
            "    >>> df.repartition(10).rdd.getNumPartitions()\n",
            "    10\n",
            "    \n",
            "    Repartition the data into 7 partitions by 'age' column.\n",
            "    \n",
            "    >>> df.repartition(7, \"age\").rdd.getNumPartitions()\n",
            "    7\n",
            "    \n",
            "    Repartition the data into 7 partitions by 'age' and 'name columns.\n",
            "    \n",
            "    >>> df.repartition(3, \"name\", \"age\").rdd.getNumPartitions()\n",
            "    3\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.createDataFrame([(1, \"Goku\"), (2, \"Naruto\")], schema=[\"id\", \"name\"])\n",
        "\n",
        "df.show()\n",
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8d868f8-2e17-4fed-9b01-f942c573c0c3",
        "id": "Sna0-VLUHBwa"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+\n",
            "| id|  name|\n",
            "+---+------+\n",
            "|  1|  Goku|\n",
            "|  2|Naruto|\n",
            "+---+------+\n",
            "\n",
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_1part = df.repartition(1)\n",
        "df_1part.rdd.getNumPartitions()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7282c62c-cb0b-4680-e2f7-44d428b46559",
        "id": "XxBZC8laHBwa"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_1part\\\n",
        ".write\\\n",
        ".csv(\"df_csv_1part\", mode=\"overwrite\", header=True)"
      ],
      "metadata": {
        "id": "eQHjXI73IDcB"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.read\\\n",
        ".option(\"header\", True)\\\n",
        ".format(\"csv\")\\\n",
        ".load(\"df_csv_1part\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZxP6Gn4cHIEx",
        "outputId": "c70b6353-cb8a-4b7b-b3f9-1f393414c2c9"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+\n",
            "| id|  name|\n",
            "+---+------+\n",
            "|  1|  Goku|\n",
            "|  2|Naruto|\n",
            "+---+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -la df_csv_1part/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "StNa-58uHWuh",
        "outputId": "3970ddb9-59e9-4b14-cebd-56f45e5a58e7"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 20\n",
            "drwxr-xr-x 2 root root 4096 Oct  9 11:20 .\n",
            "drwxr-xr-x 1 root root 4096 Oct  9 11:20 ..\n",
            "-rw-r--r-- 1 root root   24 Oct  9 11:20 part-00000-c2d5b4b6-26cb-44db-94ff-cd659fe52fc8-c000.csv\n",
            "-rw-r--r-- 1 root root   12 Oct  9 11:20 .part-00000-c2d5b4b6-26cb-44db-94ff-cd659fe52fc8-c000.csv.crc\n",
            "-rw-r--r-- 1 root root    0 Oct  9 11:20 _SUCCESS\n",
            "-rw-r--r-- 1 root root    8 Oct  9 11:20 ._SUCCESS.crc\n"
          ]
        }
      ]
    }
  ]
}