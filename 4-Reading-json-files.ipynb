{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN87SuMb4aUwftLoFMQUbjw"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Based on [WafaStudies](https://www.youtube.com/@WafaStudies) PySpark [tutorial](https://www.youtube.com/playlist?list=PLMWaZteqtEaJFiJ2FyIKK0YEuXwQ9YIS_)."
      ],
      "metadata": {
        "id": "CmlEeVvys-2Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "DnQUnkcLjqcA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz\n",
        "!tar xf spark-3.5.0-bin-hadoop3.tgz\n",
        "!pip -q install findspark"
      ],
      "metadata": {
        "id": "AMJqMeKMEby2"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.0-bin-hadoop3\""
      ],
      "metadata": {
        "id": "YCAEgkYiEdV2"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "hbpmyFRaEeyf"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "spark = SparkSession.builder\\\n",
        "                    .appName('Spark')\\\n",
        "                    .master(\"local[*]\")\\\n",
        "                    .getOrCreate()"
      ],
      "metadata": {
        "id": "J-69JCOeEgVZ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate data"
      ],
      "metadata": {
        "id": "G1cMSr_77dST"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faker"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfY3rPwv7am1",
        "outputId": "2ea2ab84-5a04-47f3-a806-39497cd6a044"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faker\n",
            "  Downloading Faker-19.10.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/1.7 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.6/1.7 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.4 in /usr/local/lib/python3.10/dist-packages (from faker) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.4->faker) (1.16.0)\n",
            "Installing collected packages: faker\n",
            "Successfully installed faker-19.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir data\n",
        "!mkdir ml_data"
      ],
      "metadata": {
        "id": "HlCq4vdS9BUd"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import random\n",
        "from faker import Faker\n",
        "\n",
        "faker = Faker('pt_BR')\n",
        "\n",
        "# Generate data for employees1.json\n",
        "with open('data/employees1.json', 'w') as jsonfile:\n",
        "    for id in range(1, 6):\n",
        "        name = faker.name()\n",
        "        salary = random.randint(1000, 10000)\n",
        "        employee = {'id': id, 'name': name, 'salary': float(salary)}\n",
        "        json.dump(employee, jsonfile)\n",
        "        jsonfile.write('\\n')\n",
        "\n",
        "# Generate data for employees2.json\n",
        "employees2_data = []\n",
        "for id in range(1, 6):\n",
        "    name = faker.name()\n",
        "    salary = random.randint(1000, 10000)\n",
        "    employee = {'id': id, 'name': name, 'salary': float(salary)}\n",
        "    employees2_data.append(employee)\n",
        "\n",
        "with open('ml_data/employees2.json', 'w') as jsonfile:\n",
        "    json.dump(employees2_data, jsonfile, indent=2)\n",
        "\n",
        "# Generate data for employees3.json\n",
        "with open('data/employees3.json', 'w') as jsonfile:\n",
        "    for id in range(6, 11):\n",
        "        name = faker.name()\n",
        "        salary = random.randint(1000, 10000)\n",
        "        employee = {'id': id, 'name': name, 'salary': float(salary)}\n",
        "        json.dump(employee, jsonfile)\n",
        "        jsonfile.write('\\n')"
      ],
      "metadata": {
        "id": "RQCXHeyy7hR1"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reading json files"
      ],
      "metadata": {
        "id": "t9lNQOajz_hy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "help(spark.read.json)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G36XnXXH0Bep",
        "outputId": "14366c6d-b13e-4103-9c47-a199708d447b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on method json in module pyspark.sql.readwriter:\n",
            "\n",
            "json(path: Union[str, List[str], pyspark.rdd.RDD[str]], schema: Union[pyspark.sql.types.StructType, str, NoneType] = None, primitivesAsString: Union[bool, str, NoneType] = None, prefersDecimal: Union[bool, str, NoneType] = None, allowComments: Union[bool, str, NoneType] = None, allowUnquotedFieldNames: Union[bool, str, NoneType] = None, allowSingleQuotes: Union[bool, str, NoneType] = None, allowNumericLeadingZero: Union[bool, str, NoneType] = None, allowBackslashEscapingAnyCharacter: Union[bool, str, NoneType] = None, mode: Optional[str] = None, columnNameOfCorruptRecord: Optional[str] = None, dateFormat: Optional[str] = None, timestampFormat: Optional[str] = None, multiLine: Union[bool, str, NoneType] = None, allowUnquotedControlChars: Union[bool, str, NoneType] = None, lineSep: Optional[str] = None, samplingRatio: Union[str, float, NoneType] = None, dropFieldIfAllNull: Union[bool, str, NoneType] = None, encoding: Optional[str] = None, locale: Optional[str] = None, pathGlobFilter: Union[bool, str, NoneType] = None, recursiveFileLookup: Union[bool, str, NoneType] = None, modifiedBefore: Union[bool, str, NoneType] = None, modifiedAfter: Union[bool, str, NoneType] = None, allowNonNumericNumbers: Union[bool, str, NoneType] = None) -> 'DataFrame' method of pyspark.sql.readwriter.DataFrameReader instance\n",
            "    Loads JSON files and returns the results as a :class:`DataFrame`.\n",
            "    \n",
            "    `JSON Lines <http://jsonlines.org/>`_ (newline-delimited JSON) is supported by default.\n",
            "    For JSON (one record per file), set the ``multiLine`` parameter to ``true``.\n",
            "    \n",
            "    If the ``schema`` parameter is not specified, this function goes\n",
            "    through the input once to determine the input schema.\n",
            "    \n",
            "    .. versionadded:: 1.4.0\n",
            "    \n",
            "    .. versionchanged:: 3.4.0\n",
            "        Supports Spark Connect.\n",
            "    \n",
            "    Parameters\n",
            "    ----------\n",
            "    path : str, list or :class:`RDD`\n",
            "        string represents path to the JSON dataset, or a list of paths,\n",
            "        or RDD of Strings storing JSON objects.\n",
            "    schema : :class:`pyspark.sql.types.StructType` or str, optional\n",
            "        an optional :class:`pyspark.sql.types.StructType` for the input schema or\n",
            "        a DDL-formatted string (For example ``col0 INT, col1 DOUBLE``).\n",
            "    \n",
            "    Other Parameters\n",
            "    ----------------\n",
            "    Extra options\n",
            "        For the extra options, refer to\n",
            "        `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-json.html#data-source-option>`_\n",
            "        for the version you use.\n",
            "    \n",
            "        .. # noqa\n",
            "    \n",
            "    Examples\n",
            "    --------\n",
            "    Write a DataFrame into a JSON file and read it back.\n",
            "    \n",
            "    >>> import tempfile\n",
            "    >>> with tempfile.TemporaryDirectory() as d:\n",
            "    ...     # Write a DataFrame into a JSON file\n",
            "    ...     spark.createDataFrame(\n",
            "    ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]\n",
            "    ...     ).write.mode(\"overwrite\").format(\"json\").save(d)\n",
            "    ...\n",
            "    ...     # Read the JSON file as a DataFrame.\n",
            "    ...     spark.read.json(d).show()\n",
            "    +---+------------+\n",
            "    |age|        name|\n",
            "    +---+------------+\n",
            "    |100|Hyukjin Kwon|\n",
            "    +---+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.json(\"data/employees1.json\")\n",
        "df.printSchema()\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48kFm-Bc0CUq",
        "outputId": "5e91e697-e332-4cfa-ddf2-db59a5271dfa"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- salary: double (nullable = true)\n",
            "\n",
            "+---+--------------------+------+\n",
            "| id|                name|salary|\n",
            "+---+--------------------+------+\n",
            "|  1|       Camila Aragão|5609.0|\n",
            "|  2|    Fernando Cardoso|4326.0|\n",
            "|  3|    Dra. Maitê Pinto|3183.0|\n",
            "|  4|Srta. Bianca Almeida|8563.0|\n",
            "|  5|   Catarina Nogueira|9706.0|\n",
            "+---+--------------------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also tell spark the schema:"
      ],
      "metadata": {
        "id": "FkqByZCK8HUr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "schema = \"id long, name string, salary double\""
      ],
      "metadata": {
        "id": "MzcnUpy-9sMb"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.json(\"data/employees1.json\", schema=schema)\n",
        "df.printSchema()\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eE62M-GP91Dx",
        "outputId": "7aba8529-89e5-40bc-885d-a354aa886b79"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- salary: double (nullable = true)\n",
            "\n",
            "+---+--------------------+------+\n",
            "| id|                name|salary|\n",
            "+---+--------------------+------+\n",
            "|  1|       Camila Aragão|5609.0|\n",
            "|  2|    Fernando Cardoso|4326.0|\n",
            "|  3|    Dra. Maitê Pinto|3183.0|\n",
            "|  4|Srta. Bianca Almeida|8563.0|\n",
            "|  5|   Catarina Nogueira|9706.0|\n",
            "+---+--------------------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What if the json file is multiline?"
      ],
      "metadata": {
        "id": "C16kTdd7-pjf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('ml_data/employees2.json', 'r') as file:\n",
        "    pretty_json = json.dumps(json.load(file), indent=4)\n",
        "    print(pretty_json)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MEG0P_Y9_15g",
        "outputId": "ad9ecf77-a4a6-4798-a525-764f50e8a9f8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\n",
            "    {\n",
            "        \"id\": 1,\n",
            "        \"name\": \"Manuela das Neves\",\n",
            "        \"salary\": 7116.0\n",
            "    },\n",
            "    {\n",
            "        \"id\": 2,\n",
            "        \"name\": \"Maysa Lopes\",\n",
            "        \"salary\": 7583.0\n",
            "    },\n",
            "    {\n",
            "        \"id\": 3,\n",
            "        \"name\": \"Ryan Nogueira\",\n",
            "        \"salary\": 6965.0\n",
            "    },\n",
            "    {\n",
            "        \"id\": 4,\n",
            "        \"name\": \"Mariana Santos\",\n",
            "        \"salary\": 9330.0\n",
            "    },\n",
            "    {\n",
            "        \"id\": 5,\n",
            "        \"name\": \"Kevin Barbosa\",\n",
            "        \"salary\": 4167.0\n",
            "    }\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try to read it:"
      ],
      "metadata": {
        "id": "vTGmkYuKAhiz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.json(\"ml_data/employees2.json\")\n",
        "df.printSchema()\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529
        },
        "id": "0XfzB149-xIW",
        "outputId": "272b9835-b6af-47ac-f43e-d2d7d32206ce"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- _corrupt_record: string (nullable = true)\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-81623b87ca28>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ml_data/employees2.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprintSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/spark-3.5.0-bin-hadoop3/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    957\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 959\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    960\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    961\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.5.0-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.5.0-bin-hadoop3/python/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: Since Spark 2.3, the queries from raw JSON/CSV files are disallowed when the\nreferenced columns only include the internal corrupt record column\n(named _corrupt_record by default). For example:\nspark.read.schema(schema).csv(file).filter($\"_corrupt_record\".isNotNull).count()\nand spark.read.schema(schema).csv(file).select(\"_corrupt_record\").show().\nInstead, you can cache or save the parsed results and then send the same query.\nFor example, val df = spark.read.schema(schema).csv(file).cache() and then\ndf.filter($\"_corrupt_record\".isNotNull).count()."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We got an error"
      ],
      "metadata": {
        "id": "2iLUz9pG_D4o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To fix it, we need to tell spark the json file is multiline:"
      ],
      "metadata": {
        "id": "COBd0cCy-0uA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.json(\"ml_data/employees2.json\", multiLine=True)\n",
        "df.printSchema()\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9j-gJIGr_KX3",
        "outputId": "978e7fad-13be-4b4e-9f2f-5598e3f40677"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- salary: double (nullable = true)\n",
            "\n",
            "+---+-----------------+------+\n",
            "| id|             name|salary|\n",
            "+---+-----------------+------+\n",
            "|  1|Manuela das Neves|7116.0|\n",
            "|  2|      Maysa Lopes|7583.0|\n",
            "|  3|    Ryan Nogueira|6965.0|\n",
            "|  4|   Mariana Santos|9330.0|\n",
            "|  5|    Kevin Barbosa|4167.0|\n",
            "+---+-----------------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What if we want to load multiple json files?"
      ],
      "metadata": {
        "id": "HQqsqFGZBjGp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Just use a list :)"
      ],
      "metadata": {
        "id": "NVjJqes8N5Rd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.json([\"data/employees1.json\", \"data/employees3.json\"])\n",
        "df.printSchema()\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GjnNYJKbAsNA",
        "outputId": "03ced1cf-41c6-4dc2-9b36-5d095463bfca"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- salary: double (nullable = true)\n",
            "\n",
            "+---+--------------------+------+\n",
            "| id|                name|salary|\n",
            "+---+--------------------+------+\n",
            "|  1|       Camila Aragão|5609.0|\n",
            "|  2|    Fernando Cardoso|4326.0|\n",
            "|  3|    Dra. Maitê Pinto|3183.0|\n",
            "|  4|Srta. Bianca Almeida|8563.0|\n",
            "|  5|   Catarina Nogueira|9706.0|\n",
            "|  6|    Lucca Nascimento|6946.0|\n",
            "|  7|       Helena Novaes|6837.0|\n",
            "|  8|      Clarice Castro|9509.0|\n",
            "|  9|Carlos Eduardo Re...|3886.0|\n",
            "| 10|        Luna Pereira|7069.0|\n",
            "+---+--------------------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If all the files are in the same folder, you can pass ```*.json```:"
      ],
      "metadata": {
        "id": "9HyIC6lKOhwD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.json(\"data/*.json\")\n",
        "df.printSchema()\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52FEoWQtBwJR",
        "outputId": "f4a4f2b2-b68f-4e29-c6ee-6fb36cef5bbf"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- salary: double (nullable = true)\n",
            "\n",
            "+---+--------------------+------+\n",
            "| id|                name|salary|\n",
            "+---+--------------------+------+\n",
            "|  1|       Camila Aragão|5609.0|\n",
            "|  2|    Fernando Cardoso|4326.0|\n",
            "|  3|    Dra. Maitê Pinto|3183.0|\n",
            "|  4|Srta. Bianca Almeida|8563.0|\n",
            "|  5|   Catarina Nogueira|9706.0|\n",
            "|  6|    Lucca Nascimento|6946.0|\n",
            "|  7|       Helena Novaes|6837.0|\n",
            "|  8|      Clarice Castro|9509.0|\n",
            "|  9|Carlos Eduardo Re...|3886.0|\n",
            "| 10|        Luna Pereira|7069.0|\n",
            "+---+--------------------+------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}